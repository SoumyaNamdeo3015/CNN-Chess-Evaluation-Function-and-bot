{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10628117,"sourceType":"datasetVersion","datasetId":6580458},{"sourceId":10628982,"sourceType":"datasetVersion","datasetId":6581061},{"sourceId":10630263,"sourceType":"datasetVersion","datasetId":6581848},{"sourceId":246762,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":210879,"modelId":232569},{"sourceId":247323,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":211375,"modelId":233073}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:23.763579Z","iopub.execute_input":"2025-02-01T10:20:23.763885Z","iopub.status.idle":"2025-02-01T10:20:23.794984Z","shell.execute_reply.started":"2025-02-01T10:20:23.763858Z","shell.execute_reply":"2025-02-01T10:20:23.794179Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/loadmodel/keras/default/1/model_8814_42.h5\n/kaggle/input/test-data/positions_processed.csv\n/kaggle/input/test-data-set/test_track_8.csv\n/kaggle/input/training-data/Chess_1.csv\n/kaggle/input/training-data/Chess_3.csv\n/kaggle/input/training-data/Chess_2.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## scope for improvement\n\n* add king score depnding on the game pahase\n* train the current model on chess reduced\n  \n\n","metadata":{}},{"cell_type":"markdown","source":"##  sumbission entry:\n\n1. (first submission) submission2.csv --> used model 8814_42, trained Chess1.csv, main epoch = 3","metadata":{}},{"cell_type":"markdown","source":"## Import necessary modules","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow\nimport sklearn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:23.796117Z","iopub.execute_input":"2025-02-01T10:20:23.796336Z","iopub.status.idle":"2025-02-01T10:20:37.767785Z","shell.execute_reply.started":"2025-02-01T10:20:23.796318Z","shell.execute_reply":"2025-02-01T10:20:37.767135Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Some board pre processing functions ","metadata":{}},{"cell_type":"code","source":"piece_to_col = {\n    'P':0, 'B':1,'N':2,'R':3,'Q':4,'K':5,\n    'p':6, 'b':7,'n':8,'r':9,'q':10,'k':11,\n}\n\ncol_to_piece = {\n    val:key for key, val in piece_to_col.items()\n}\ndef getBoard(fen):\n    fen_str = fen.split(' ')[0]\n    ranks = fen_str.split('/')\n    board = []\n    for i in range(8):\n        rank = ranks[i]\n        board_rank = \"\"\n        for j in rank:\n            if(j.isdigit()):\n                board_rank += '.'*int(j)\n\n            else:\n                board_rank += j\n        board.append(board_rank)\n    return board\n\n# print(getBoard(\"r2q1rk1/pb3pbp/1ppp1np1/2nPp3/4P3/2P1B2P/PPBNNPP1/R2Q1RK1 w - - 0 12\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.769576Z","iopub.execute_input":"2025-02-01T10:20:37.770048Z","iopub.status.idle":"2025-02-01T10:20:37.775330Z","shell.execute_reply.started":"2025-02-01T10:20:37.770026Z","shell.execute_reply":"2025-02-01T10:20:37.774468Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def convert_fen(fen):\n    ls = fen.split(' ')\n\n    if(ls[1] == 'w'): #activeColor = 'w'\n        return fen\n\n    #if active color is 'b' then we need to convert it to 'w'\n    fen_str = ls[0]\n    fen_str = fen_str[::-1]\n    new_fen_str = \"\"\n\n    for char in fen_str:\n        if(char == '/' or (char.isdigit())):\n            new_fen_str += char\n        else:\n            if(char.islower()):\n                new_fen_str += char.upper()\n            else:\n                new_fen_str += char.lower()\n\n    new_fen_ls = new_fen_str.split('/')\n    new_fen_tmp = ''\n    for rank in new_fen_ls :\n        new_fen_tmp += rank[::-1]\n        new_fen_tmp += '/'\n\n    new_fen_str = new_fen_tmp[:-1]\n\n    cas_str = ls[2]\n\n    new_cas_str = \"\"\n\n    for char in cas_str:\n        if(char == '-'):\n            new_cas_str += char\n        else:\n            if(char.islower()):\n                new_cas_str += char.upper()\n            else:\n                new_cas_str += char.lower()\n\n\n    cas_str1 = \"\"\n\n    if(new_cas_str == \"-\"):\n      cas_str1 = \"-\"\n\n    else:\n\n      if('K' in new_cas_str):\n        cas_str1 += \"K\"\n\n      if(\"Q\" in new_cas_str):\n        cas_str1 += \"Q\"\n\n      if('k' in new_cas_str):\n        cas_str1 += \"k\"\n      if(\"q\" in new_cas_str):\n        cas_str1 += \"q\"\n\n\n\n    ep = ls[3]\n    if(ep != '-') :\n        c2 = int(ep[1]) - 1\n        c2 = 8 - c2\n        ep = ep[:-1]\n        ep += str(c2)\n\n\n    new_ls = [new_fen_str, 'w', cas_str1, ep ,ls[4], ls[5]]\n    new_fen = \"\"\n    for i in range(5):\n        new_fen += new_ls[i]\n        new_fen += \" \"\n    new_fen += new_ls[5]\n\n    return new_fen","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.776298Z","iopub.execute_input":"2025-02-01T10:20:37.776553Z","iopub.status.idle":"2025-02-01T10:20:37.820688Z","shell.execute_reply.started":"2025-02-01T10:20:37.776522Z","shell.execute_reply":"2025-02-01T10:20:37.819934Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\npiece_to_col = {\n    'P':0, 'B':1,'N':2,'R':3,'Q':4,'K':5,\n    'p':6, 'b':7,'n':8,'r':9,'q':10,'k':11,\n}\n\ndef getWhitePawnMoves(board, x, y):\n    moves = []\n\n    if x > 0 and board[x - 1][y] == '.':\n        moves.append([x - 1, y])\n\n        if x == 6 and board[x - 2][y] == '.':\n            moves.append([x - 2, y])\n\n    if x > 0 and y > 0 and board[x - 1][y - 1].islower():\n        moves.append([x - 1, y - 1])\n\n    if x > 0 and y < 7 and board[x - 1][y + 1].islower():\n        moves.append([x - 1, y + 1])\n\n    return moves\n\n\ndef getBlackPawnMoves(board, x, y):\n    moves = []\n\n   \n    if x < 7 and board[x + 1][y] == '.':\n        moves.append([x + 1, y])\n\n        \n        if x == 1 and board[x + 2][y] == '.':\n            moves.append([x + 2, y])\n\n    \n    if x < 7 and y > 0 and board[x + 1][y - 1].isupper():\n        moves.append([x + 1, y - 1])\n\n   \n    if x < 7 and y < 7 and board[x + 1][y + 1].isupper():\n        moves.append([x + 1, y + 1])\n\n    return moves\n\n\ndef getKnightMoves(board, x, y, is_white):\n    moves = []\n    directions = [\n        (-2, -1), (-2, 1), (-1, -2), (-1, 2),\n        (1, -2), (1, 2), (2, -1), (2, 1)\n    ]\n\n    for dx, dy in directions:\n        new_x, new_y = x + dx, y + dy\n        if 0 <= new_x < 8 and 0 <= new_y < 8:\n            if board[new_x][new_y] == '.' or (is_white and board[new_x][new_y].islower()) or (not is_white and board[new_x][new_y].isupper()):\n                moves.append([new_x, new_y])\n\n    return moves\n\n\ndef getSlidingMoves(board, x, y, directions, is_white):\n    moves = []\n\n    for dx, dy in directions:\n        new_x, new_y = x + dx, y + dy\n        while 0 <= new_x < 8 and 0 <= new_y < 8:\n            if board[new_x][new_y] == '.':\n                moves.append([new_x, new_y])\n            elif (is_white and board[new_x][new_y].islower()) or (not is_white and board[new_x][new_y].isupper()):\n                moves.append([new_x, new_y])\n                break\n            else:\n                break\n            new_x += dx\n            new_y += dy\n\n    return moves\n\n\ndef getBishopMoves(board, x, y, is_white):\n    return getSlidingMoves(board, x, y, [(-1, -1), (-1, 1), (1, -1), (1, 1)], is_white)\n\n\ndef getRookMoves(board, x, y, is_white):\n    return getSlidingMoves(board, x, y, [(-1, 0), (1, 0), (0, -1), (0, 1)], is_white)\n\n\ndef getQueenMoves(board, x, y, is_white):\n    return getSlidingMoves(board, x, y,\n        [(-1, -1), (-1, 1), (1, -1), (1, 1), (-1, 0), (1, 0), (0, -1), (0, 1)], is_white)\n\n\ndef isSquareAttacked(board, x, y, is_white):\n    \"\"\"Check if the square (x, y) is attacked by an opponent.\"\"\"\n    for i in range(8):\n        for j in range(8):\n            piece = board[i][j]\n            if piece == '.':\n                continue\n            if (is_white and piece.islower()) or (not is_white and piece.isupper()):\n                if piece.lower() == 'p':  # Check pawn attacks\n                    if is_white and (i == x + 1 and (j == y + 1 or j == y - 1)):\n                        return True\n                    if not is_white and (i == x - 1 and (j == y + 1 or j == y - 1)):\n                        return True\n                elif piece.lower() == 'n':  \n                    if [x, y] in getKnightMoves(board, i, j, not is_white):\n                        return True\n                elif piece.lower() == 'b': \n                    if [x, y] in getBishopMoves(board, i, j, not is_white):\n                        return True\n                elif piece.lower() == 'r':  \n                    if [x, y] in getRookMoves(board, i, j, not is_white):\n                        return True\n                elif piece.lower() == 'q':  \n                    if [x, y] in getQueenMoves(board, i, j, not is_white):\n                        return True\n                elif piece.lower() == 'k': \n                    if abs(i - x) <= 1 and abs(j - y) <= 1:\n                        return True\n    return False\n\n\ndef getKingMoves(board, x, y, is_white):\n    moves = []\n    directions = [\n        (-1, -1), (-1, 0), (-1, 1), (0, -1),\n        (0, 1), (1, -1), (1, 0), (1, 1)\n    ]\n\n    for dx, dy in directions:\n        new_x, new_y = x + dx, y + dy\n        if 0 <= new_x < 8 and 0 <= new_y < 8:\n            if board[new_x][new_y] == '.' or (is_white and board[new_x][new_y].islower()) or (not is_white and board[new_x][new_y].isupper()):\n                if not isSquareAttacked(board, new_x, new_y, is_white):  # King cannot move into check\n                    moves.append([new_x, new_y])\n\n    return moves\n\n\ndef getMoves(board , piece , x , y) :\n    moves = []\n    is_white = True\n    if(piece.islower()) :\n        is_white = False\n\n    if(piece == 'P') :\n        moves = getWhitePawnMoves(board , x , y)\n    elif(piece == 'p') :\n        moves = getBlackPawnMoves(board, x, y)\n    elif(piece == 'N' or piece == 'n') :\n        moves = getKnightMoves(board, x, y, is_white)\n    elif(piece == 'B' or piece == 'b') :\n        moves = getBishopMoves(board, x, y, is_white)\n    elif(piece == 'R' or piece == 'r') :\n        moves = getRookMoves(board, x, y, is_white)\n    elif(piece == 'Q' or piece == 'q') :\n        moves = getQueenMoves(board, x, y, is_white)\n    elif(piece == 'K' or piece == 'k') :\n        moves = getKingMoves(board, x, y, is_white)\n\n    return moves\n\n\n\ndef getAttackingSquares(board) :\n    white_attack = np.zeros((8 , 8))\n    black_attack = np.zeros((8 , 8))\n\n    for i in range(8) :\n        for j in range(8) :\n            piece = board[i][j]\n            if(piece == '.') :\n                continue\n            if(piece.isupper()) :\n                moves = getMoves(board , piece , i , j)\n                for move in moves :\n                    white_attack[move[0]][move[1]] = 1\n\n            else :\n                moves = getMoves(board , piece , i , j)\n                for move in moves :\n                    black_attack[move[0]][move[1]] = 1\n\n    return white_attack , black_attack\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.821517Z","iopub.execute_input":"2025-02-01T10:20:37.821870Z","iopub.status.idle":"2025-02-01T10:20:37.846649Z","shell.execute_reply.started":"2025-02-01T10:20:37.821839Z","shell.execute_reply":"2025-02-01T10:20:37.846002Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Encoding chess board into 8x8x14","metadata":{}},{"cell_type":"code","source":"def encoding8814(fen):\n\n    fen_str = fen.split(' ')[0]\n    board_encoding = np.zeros((8,8,14))\n    # print(board_encoding)\n    board = getBoard(fen)\n    # print(board)\n\n    for i in range(8):\n        row = board[i]\n        # print(row)\n        for j in range(8):\n            piece = row[j]\n            # print(piece)\n            if(piece != '.'):\n                board_encoding[i][j][piece_to_col[piece]] = 1\n\n\n    white_legal, black_legal = getAttackingSquares(board)\n\n    board_encoding[:,:,12] = white_legal\n    board_encoding[:,:,13] = black_legal\n\n    return board_encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.847291Z","iopub.execute_input":"2025-02-01T10:20:37.847471Z","iopub.status.idle":"2025-02-01T10:20:37.866443Z","shell.execute_reply.started":"2025-02-01T10:20:37.847454Z","shell.execute_reply":"2025-02-01T10:20:37.865726Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def active_color(fen):\n    color = fen.split(' ')[1]\n    return color\n\ndef flip_eval(row):\n    fen = row['FEN']\n    color = active_color(fen)\n    if(color == 'w'):\n        return row['Evaluation']\n    else:\n        return -row['Evaluation']\n\ndef processData(df):\n    df['Evaluation'] = df.apply(flip_eval, axis = 1)\n    df['FEN'] = df['FEN'].apply(convert_fen)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.867148Z","iopub.execute_input":"2025-02-01T10:20:37.867330Z","iopub.status.idle":"2025-02-01T10:20:37.883489Z","shell.execute_reply.started":"2025-02-01T10:20:37.867313Z","shell.execute_reply":"2025-02-01T10:20:37.882769Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Additional features","metadata":{}},{"cell_type":"code","source":"def rookFeatures(board):\n  '''\n    return [count of rooks on 7th rank, number of open files controlled by rook,\n    rook battery, mobility]\n  '''\n  white_rook_count_on_7th_rank = 0\n  white_open_files_controlled = 0\n  white_semi_open_files_controlled = 0\n  white_mobility = 0\n\n  black_rook_count_on_7th_rank = 0\n  black_open_files_controlled = 0\n  black_semi_open_files_controlled = 0\n  black_mobility = 0\n\n  white_battery = 0\n  black_battery = 0\n\n  white_rook_positions = []\n  black_rook_positions = []\n\n  for i in range(8):\n    for j in range(8):\n      if(board[i][j] == 'R'):\n        white_rook_positions.append([i,j])\n      if(board[i][j] == 'r'):\n        black_rook_positions.append([i,j])\n\n  # print(white_rook_positions)\n\n  for position in white_rook_positions:\n\n    if(position[0] == 1):\n      white_rook_count_on_7th_rank += 1\n\n    occupied_by_white_pawn = False\n    occupied_by_black_pawn = False\n    i = position[0] #rank\n    j = position[1] #file\n    # print(\"white rook at\",i,j)\n    legal_moves = getRookMoves(board, i, j, 1)\n    # print(legal_moves)\n    white_mobility += len(legal_moves)\n\n    for rank in range(8):\n      if(rank != i):\n        if(board[rank][j] == 'P'):\n          occupied_by_white_pawn = True\n        if(board[rank][j] == 'p'):\n          occupied_by_black_pawn = True\n\n    # print(occupied_by_white_pawn)\n    # print(occupied_by_black_pawn)\n\n    if((not occupied_by_white_pawn) and (not occupied_by_black_pawn)):\n        white_open_files_controlled += 1\n    if((not occupied_by_white_pawn) and (occupied_by_black_pawn)):\n        white_semi_open_files_controlled += 1\n\n  for position in black_rook_positions:\n\n      if(position[0] == 6):\n        black_rook_count_on_7th_rank += 1\n\n      occupied_by_white_pawn = False\n      occupied_by_black_pawn = False\n      i = position[0] #rank\n      j = position[1] #file\n\n      legal_moves = getRookMoves(board, i, j, 0)\n      black_mobility += len(legal_moves)\n\n      for rank in range(8):\n        if(rank != i):\n          if(board[rank][j] == 'P'):\n            occupied_by_white_pawn = True\n          if(board[rank][j] == 'p'):\n            occupied_by_black_pawn = True\n\n      if((not occupied_by_white_pawn) and (not occupied_by_black_pawn)):\n        black_open_files_controlled += 1\n      if((not occupied_by_black_pawn) and (occupied_by_white_pawn)):\n        black_semi_open_files_controlled += 1\n\n\n  if(len(white_rook_positions) == 2):\n    rook1_i = white_rook_positions[0][0]\n    rook1_j = white_rook_positions[0][1]\n\n    rook2_i = white_rook_positions[1][0]\n    rook2_j = white_rook_positions[1][1]\n\n    white_battery = True\n\n    if(rook1_i == rook2_i):\n      min_file = min(rook1_j, rook2_j)\n      max_file = max(rook1_j, rook2_j)\n      for j in range(min_file+1, max_file):\n        if(board[rook1_i][j] != '.'):\n          white_battery = False\n\n\n    if(rook1_j == rook2_j):\n      min_rank = min(rook1_i, rook2_i)\n      max_rank = max(rook1_i, rook2_i)\n      for i in range(min_rank+1, max_rank):\n        if(board[i][rook1_j] != '.'):\n          white_battery = False\n\n\n  if(len(black_rook_positions) == 2):\n    rook1_i = black_rook_positions[0][0]\n    rook1_j = black_rook_positions[0][1]\n\n    rook2_i = black_rook_positions[1][0]\n    rook2_j = black_rook_positions[1][1]\n\n\n    black_battery = True\n\n    if(rook1_i == rook2_i):\n\n      min_file = min(rook1_j, rook2_j)\n      max_file = max(rook1_j, rook2_j)\n      for j in range(min_file+1, max_file):\n        # print(board[rook1_i][j])\n        if(board[rook1_i][j] != '.'):\n          black_battery = False\n    # print(black_battery)\n\n    if(rook1_j == rook2_j):\n      min_rank = min(rook1_i, rook2_i)\n      max_rank = max(rook1_i, rook2_i)\n      for i in range(min_rank+1, max_rank):\n        if(board[i][rook1_j] != '.'):\n          black_battery = False\n\n\n\n  return [\n      [white_rook_count_on_7th_rank, white_open_files_controlled, white_semi_open_files_controlled, white_battery, white_mobility],\n      [black_rook_count_on_7th_rank, black_open_files_controlled, black_semi_open_files_controlled, black_battery, black_mobility]\n\n  ]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.884291Z","iopub.execute_input":"2025-02-01T10:20:37.884569Z","iopub.status.idle":"2025-02-01T10:20:37.897959Z","shell.execute_reply.started":"2025-02-01T10:20:37.884539Z","shell.execute_reply":"2025-02-01T10:20:37.897400Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def BishopFeatures(board):\n\n  '''\n    [bishop pair, bishop in center,  mobility] for white and black\n  '''\n  white_bishop_positions = []\n  black_bishop_positions = []\n\n  for rank in range(8):\n    for file in range(8):\n      piece = board[rank][file]\n      if(piece == 'B'):\n        white_bishop_positions.append([rank, file])\n      if(piece == 'b'):\n        black_bishop_positions.append([rank, file])\n\n  white_mobility = 0\n  black_mobility = 0\n  white_bishop_pair = 0\n  black_bishop_pair = 0\n  white_bishop_on_center = 0\n  black_bishop_on_center = 0\n\n  if(len(white_bishop_positions) >= 2):\n    white_bishop_pair = 1\n  if(len(black_bishop_positions) >= 2):\n    black_bishop_pair = 1\n\n  center_squares = [[3,3], [3,4], [4,3], [4,4]]\n\n  for position in white_bishop_positions:\n    legal_moves = getBishopMoves(board, position[0], position[1], 1)\n    white_mobility += len(legal_moves)\n    if(position in center_squares):\n      white_bishop_on_center += 1\n\n\n  for position in black_bishop_positions:\n    legal_moves = getBishopMoves(board, position[0], position[1], 0)\n    black_mobility += len(legal_moves)\n    if(position in center_squares):\n      black_bishop_on_center += 1\n\n\n  return [\n      [white_bishop_pair, white_bishop_on_center, white_mobility],\n      [black_bishop_pair, black_bishop_on_center, black_mobility]\n  ]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.900420Z","iopub.execute_input":"2025-02-01T10:20:37.900636Z","iopub.status.idle":"2025-02-01T10:20:37.914724Z","shell.execute_reply.started":"2025-02-01T10:20:37.900618Z","shell.execute_reply":"2025-02-01T10:20:37.914166Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def KnightFeatures(board):\n  '''\n    [knight on center, mobility] for white and black\n  '''\n\n  center_squares = [[3,3], [3,4], [4,3], [4,4]]\n\n  white_knight_on_center = 0\n  black_knight_on_center = 0\n\n  white_mobility = 0\n  black_mobility = 0\n\n  white_knight_outpost = 0\n  black_knight_outpost = 0\n\n  white_knight_positions = []\n  black_knight_positions = []\n\n  for rank in range(8):\n    for file in range(8):\n      if(board[rank][file] == 'N'):\n        white_knight_positions.append([rank, file])\n      if(board[rank][file] == 'n'):\n        black_knight_positions.append([rank, file])\n\n  for position in white_knight_positions:\n    if(position in center_squares):\n      white_knight_on_center += 1\n    legal_moves = getKnightMoves(board, position[0], position[1], 1)\n    white_mobility += len(legal_moves)\n\n  for position in black_knight_positions:\n    if(position in center_squares):\n      black_knight_on_center += 1\n    legal_moves = getKnightMoves(board, position[0], position[1], 0)\n    black_mobility += len(legal_moves)\n\n  white_rank_low = 1\n  white_rank_high = 4\n  white_file_low = 2\n  white_file_high = 5\n\n  black_rank_high = 6\n  black_rank_low = 3\n  black_file_low = 2\n  black_file_high = 5\n\n\n  for position in white_knight_positions:\n    rank = position[0]\n    file = position[1]\n    ok = (white_rank_low <= rank and rank <= white_rank_high) and (white_file_low <= file and file <= white_file_high)\n    if(not ok):\n      continue\n\n\n    defended = False\n    if(board[rank+1][file-1] == 'P' or board[rank+1][file + 1] == 'P'):\n      defended = True\n\n\n    can_be_attacked = False\n\n    for j in range(rank-1,0, -1):\n      if(board[j][file-1] == 'p' or board[j][file+1] == 'p'):\n        can_be_attacked = True\n\n     \n\n    if(defended and not can_be_attacked):\n      white_knight_outpost += 1\n\n  for position in black_knight_positions:\n    rank = position[0]\n    file = position[1]\n    ok = (black_rank_low <= rank and rank <= black_rank_high) and (black_file_low <= file and file <= black_file_high)\n\n    if(not ok):\n      continue\n\n\n    defended = False\n    if(board[rank-1][file-1] == 'p' or board[rank-1][file + 1] == 'p'):\n      defended = True\n\n\n    can_be_attacked = False\n\n    for j in range(rank+1,7):\n      if(board[j][file-1] == 'P' or board[j][file+1] == 'P'):\n        can_be_attacked = True\n\n    if(defended and (not can_be_attacked)):\n      black_knight_outpost += 1\n\n\n  return[\n      [white_knight_on_center, white_knight_outpost, white_mobility],\n      [black_knight_on_center,black_knight_outpost, black_mobility]\n  ]\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.916268Z","iopub.execute_input":"2025-02-01T10:20:37.916666Z","iopub.status.idle":"2025-02-01T10:20:37.937233Z","shell.execute_reply.started":"2025-02-01T10:20:37.916645Z","shell.execute_reply":"2025-02-01T10:20:37.936431Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"PAWN_TABLE = [\n    [ 0,  0,  0,  0,  0,  0,  0,  0],\n    [50, 50, 50, 50, 50, 50, 50, 50],\n    [10, 10, 20, 30, 30, 20, 10, 10],\n    [ 5,  5, 10, 25, 25, 10,  5,  5],\n    [ 0,  0,  0, 20, 20,  0,  0,  0],\n    [ 5, -5,-10,  0,  0,-10, -5,  5],\n    [ 5, 10, 10,-20,-20, 10, 10,  5],\n    [ 0,  0,  0,  0,  0,  0,  0,  0]\n]\n\nKNIGHT_TABLE = [\n    [-50, -40, -30, -30, -30, -30, -40, -50],\n    [-40, -20,   0,   0,   0,   0, -20, -40],\n    [-30,   0,  10,  15,  15,  10,   0, -30],\n    [-30,   5,  15,  20,  20,  15,   5, -30],\n    [-30,   0,  15,  20,  20,  15,   0, -30],\n    [-30,   5,  10,  15,  15,  10,   5, -30],\n    [-40, -20,   0,   5,   5,   0, -20, -40],\n    [-50, -40, -30, -30, -30, -30, -40, -50]\n]\n\nBISHOP_TABLE = [\n    [-20, -10, -10, -10, -10, -10, -10, -20],\n    [-10,   0,   0,   0,   0,   0,   0, -10],\n    [-10,   0,   5,  10,  10,   5,   0, -10],\n    [-10,   5,   5,  10,  10,   5,   5, -10],\n    [-10,   0,  10,  10,  10,  10,   0, -10],\n    [-10,  10,  10,  10,  10,  10,  10, -10],\n    [-10,   5,   0,   0,   0,   0,   5, -10],\n    [-20, -10, -10, -10, -10, -10, -10, -20]\n]\n\nROOK_TABLE = [\n    [  0,   0,   0,   0,   0,   0,   0,   0],\n    [  5,  10,  10,  10,  10,  10,  10,   5],\n    [ -5,   0,   0,   0,   0,   0,   0,  -5],\n    [ -5,   0,   0,   0,   0,   0,   0,  -5],\n    [ -5,   0,   0,   0,   0,   0,   0,  -5],\n    [ -5,   0,   0,   0,   0,   0,   0,  -5],\n    [ -5,   0,   0,   0,   0,   0,   0,  -5],\n    [  0,   0,   0,   5,   5,   0,   0,   0]\n]\n\nQUEEN_TABLE = [\n    [-20, -10, -10,  -5,  -5, -10, -10, -20],\n    [-10,   0,   0,   0,   0,   0,   0, -10],\n    [-10,   0,   5,   5,   5,   5,   0, -10],\n    [ -5,   0,   5,   5,   5,   5,   0,  -5],\n    [  0,   0,   5,   5,   5,   5,   0,  -5],\n    [-10,   5,   5,   5,   5,   5,   0, -10],\n    [-10,   0,   5,   0,   0,   0,   0, -10],\n    [-20, -10, -10,  -5,  -5, -10, -10, -20]\n]\n\nKING_MIDDLE_GAME_TABLE = [\n    [-30, -40, -40, -50, -50, -40, -40, -30],\n    [-30, -40, -40, -50, -50, -40, -40, -30],\n    [-30, -40, -40, -50, -50, -40, -40, -30],\n    [-30, -40, -40, -50, -50, -40, -40, -30],\n    [-20, -30, -30, -40, -40, -30, -30, -20],\n    [-10, -20, -20, -20, -20, -20, -20, -10],\n    [ 20,  20,   0,   0,   0,   0,  20,  20],\n    [ 20,  30,  10,   0,   0,  10,  30,  20]\n]\n\n\nKING_END_GAME_TABLE = [\n    [-50, -40, -30, -20, -20, -30, -40, -50],\n    [-30, -20, -10,   0,   0, -10, -20, -30],\n    [-30, -10,  20,  30,  30,  20, -10, -30],\n    [-30, -10,  30,  40,  40,  30, -10, -30],\n    [-30, -10,  30,  40,  40,  30, -10, -30],\n    [-30, -10,  20,  30,  30,  20, -10, -30],\n    [-30, -30,   0,   0,   0,   0, -30, -30],\n    [-50, -30, -30, -30, -30, -30, -30, -50]\n]\n\npsqt_map_middlegame = {'P':PAWN_TABLE,\n                       'N':KNIGHT_TABLE,\n                       'B':BISHOP_TABLE,\n                       'R':ROOK_TABLE,\n                       'Q':QUEEN_TABLE,\n                       'K':KING_MIDDLE_GAME_TABLE}\n\npsqt_map_endgame = {'P':PAWN_TABLE,\n                       'N':KNIGHT_TABLE,\n                       'B':BISHOP_TABLE,\n                       'R':ROOK_TABLE,\n                       'Q':QUEEN_TABLE,\n                       'K':KING_END_GAME_TABLE}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.938044Z","iopub.execute_input":"2025-02-01T10:20:37.938318Z","iopub.status.idle":"2025-02-01T10:20:37.953578Z","shell.execute_reply.started":"2025-02-01T10:20:37.938299Z","shell.execute_reply":"2025-02-01T10:20:37.952901Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\npiece_to_value = {\n    'P':100, 'p':-100, 'B':330, 'b':-330, 'N':320, 'n':-320, 'R':500, 'r':-500,\n    'Q':900, 'q':-900, 'K':50, 'k':-50, '.':0\n}\n\ndef value(peice):\n    return piece_to_value[peice]\n\ndef is_endgame_fen(fen):\n    WHITE_COUNT = 0\n    BLACK_COUNT = 0\n    fen_str = fen.split(' ')[0]\n    board = fen_str\n\n    for char in board:\n      if(char == 'p' or char == 'P' or char == 'k' or char == 'K' or char == '/' or char.isdigit()):\n        continue\n      if(char.isupper()) :\n        WHITE_COUNT += value(char)\n      else :\n        BLACK_COUNT += value(char)\n\n\n    return abs(BLACK_COUNT) <= 1350\n\ndef is_endgame(board):\n    WHITE_COUNT = 0\n    BLACK_COUNT = 0\n\n    for i in range(8):\n      for char in board[i]:\n        if(char == 'p' or char == 'P' or char == 'k' or char == 'K' or char == '/' or char.isdigit()):\n          continue\n        if(char.isupper()) :\n          WHITE_COUNT += value(char)\n        else :\n          BLACK_COUNT += value(char)\n\n    return abs(BLACK_COUNT) <= 1350\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.954302Z","iopub.execute_input":"2025-02-01T10:20:37.954566Z","iopub.status.idle":"2025-02-01T10:20:37.970662Z","shell.execute_reply.started":"2025-02-01T10:20:37.954546Z","shell.execute_reply":"2025-02-01T10:20:37.969955Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def pawn_structure_features(board) :\n    '''\n        return [double_isolated , double , isolated , central , backward]\n    '''\n\n    double_isolated_white , double_isolated_black = 0 , 0\n    double_white , double_black = 0 , 0\n    isolated_white , isolated_black = 0 , 0\n    central_white , central_black = 0 , 0\n    back_white , back_black = 0 , 0\n\n    def get_count_in_column(board , piece , j) :\n        if(j < 0 or j >= 8) :\n            return 0\n\n        count = 0\n        for i in range(8) :\n            if(board[i][j] == piece) :\n                count += 1\n        return count\n\n    white_pawn_count = [0] * 8\n    black_pawn_count = [0] * 8\n\n    for j in range(8) :\n        white_pawn_count[j] = get_count_in_column(board , 'P' , j)\n        black_pawn_count[j] = get_count_in_column(board , 'p' , j)\n\n    for j in range(8) :\n        white_prev = 0\n        if(j > 0) :\n            white_prev = white_pawn_count[j - 1]\n        white_next = 0\n        if(j < 7) :\n            white_next = white_pawn_count[j + 1]\n        white_count = white_pawn_count[j]\n\n        black_prev = 0\n        if(j > 0) :\n            black_prev = black_pawn_count[j - 1]\n        black_next = 0\n        if(j < 7) :\n            black_next = black_pawn_count[j + 1]\n        black_count = black_pawn_count[j]\n\n        if(white_count > 0) :\n            if(white_prev == 0 and white_next == 0 and white_count > 1) :\n                double_isolated_white += (white_count - 1)\n            elif(white_count > 1) :\n                double_white += (white_count - 1)\n            elif(white_prev == 0 and white_next == 0) :\n                isolated_white += 1\n\n        if(black_count > 0) :\n            if(black_prev == 0 and black_next == 0 and black_count > 1) :\n                double_isolated_black += (black_count - 1)\n            elif(black_count > 1) :\n                double_black += (black_count - 1)\n            elif(black_prev == 0 and black_next == 0) :\n                isolated_black += 1\n\n    if(board[3][3] == 'P') :\n        central_white += 1\n    if(board[4][3] == 'P') :\n        central_white += 1\n    if(board[3][4] == 'P') :\n        central_white += 1\n    if(board[4][4] == 'P') :\n        central_white += 1\n\n    if(board[3][3] == 'p') :\n        central_black += 1\n    if(board[4][3] == 'p') :\n        central_black += 1\n    if(board[3][4] == 'p') :\n        central_black += 1\n    if(board[4][4] == 'p') :\n        central_black += 1\n\n    white_rank = [8] * 8\n    black_rank = [8] * 8\n    for j in range(8) :\n        for i in range(8) :\n            if(board[i][j] == 'p') :\n                black_rank[j] = i\n                break\n\n        for i in range(8) :\n            if(board[7 - i][j] == 'P') :\n                white_rank[j] = i\n                break\n\n    for j in range(8) :\n        if(white_rank[j] < 8) :\n            prev_rank = 8\n            next_rank = 8\n            if(j > 0) :\n                prev_rank = white_rank[j - 1]\n            if(j < 7) :\n                next_rank = white_rank[j + 1]\n\n            if(white_rank[j] < prev_rank and white_rank[j] < next_rank) :\n                back_white += 1\n\n        if(black_rank[j] < 8) :\n            prev_rank = 8\n            next_rank = 8\n            if(j > 0) :\n                prev_rank = black_rank[j - 1]\n            if(j < 7) :\n                next_rank = black_rank[j + 1]\n\n            if(black_rank[j] < prev_rank and black_rank[j] < next_rank) :\n                back_black += 1\n\n    back_white -= (isolated_white + double_isolated_white)\n    back_black -= (isolated_black + double_isolated_black)\n\n    white_queen , black_queen = 0 , 0\n    white_king , black_king = 0 , 0\n\n    for j in range(4) :\n        for i in range(8) :\n            if(board[i][j] == 'P') :\n                white_queen += 1\n            elif(board[i][j] == 'p') :\n                black_queen += 1\n\n            if(board[i][7 - j] == 'P') :\n                white_king += 1\n            elif(board[i][7 - j] == 'p') :\n                black_king += 1\n\n\n\n    return [\n        [double_isolated_white, double_white, central_white, isolated_white, back_white, white_queen, white_king],\n        [double_isolated_black, double_black, central_black, isolated_black, back_black, black_queen, black_king]\n    ]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.971375Z","iopub.execute_input":"2025-02-01T10:20:37.971632Z","iopub.status.idle":"2025-02-01T10:20:37.986778Z","shell.execute_reply.started":"2025-02-01T10:20:37.971607Z","shell.execute_reply":"2025-02-01T10:20:37.986155Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def is_passed_pawn(board , x, y):\n    is_black = board[x][y] == 'p'\n    ''' Check if a pawn at (y, x) is a passed pawn. '''\n    direction = 1 if is_black else - 1\n    for dy in [-1, 0, 1]:\n        ny = y + dy\n        if 0 <= ny < 8:\n            for nx in range(x + direction, 8 if is_black else -1, direction):\n                if((is_black and board[nx][ny] == 'P') or ((not is_black) and board[nx][ny] == 'p')) :\n                    return False\n    return True\n\ndef pass_pawn_features(board):\n    '''\n    Return [connected_pp, sum of advanced squares] for both white and black.\n\n    Passed pawns are pawns with no opposing pawns in their file or adjacent files blocking their advancement.\n    Connected passed pawns are passed pawns that are adjacent to each other.\n    The sum of advanced squares is the total number of ranks each passed pawn has advanced from its starting position.\n    '''\n    white_connected, black_connected = 0, 0\n    white_promotion, black_promotion = 0, 0\n    white_passed, black_passed = [], []\n\n    for x in range(8):\n        for y in range(8):\n            if board[x][y] == 'P':  \n                if is_passed_pawn(board , x, y):\n              \n                    white_passed.append((x, y))\n                    white_promotion += (7 - x) \n            elif board[x][y] == 'p': \n                if is_passed_pawn(board , x, y):\n                    # print(x , y)\n                    black_passed.append((x, y))\n                    black_promotion += x  \n\n    for x, y in white_passed:\n        if (x, y - 1) in white_passed or (x, y + 1) in white_passed:\n            white_connected += 1\n\n    for x, y in black_passed:\n        if (x, y - 1) in black_passed or (x, y + 1) in black_passed:\n            black_connected += 1\n\n   \n    return [\n          [white_connected, white_promotion],\n          [black_connected, black_promotion]\n    ]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:37.987381Z","iopub.execute_input":"2025-02-01T10:20:37.987599Z","iopub.status.idle":"2025-02-01T10:20:38.003004Z","shell.execute_reply.started":"2025-02-01T10:20:37.987580Z","shell.execute_reply":"2025-02-01T10:20:38.002284Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def material_features(board) :\n   \n    psqt_map = psqt_map_middlegame\n    if(is_endgame(board)) :\n        psqt_map = psqt_map_endgame\n\n    white_material , black_material = 0 , 0\n\n    for i in range(8) :\n        for j in range(8) :\n            if(board[i][j] == '.') :\n                continue\n            abs_piece = board[i][j].upper()\n            if(board[i][j].isupper()) :\n                #white piece\n                white_material += (value(abs_piece) + psqt_map[abs_piece][i][j])\n            else :\n                #black piece\n                black_material += (value(abs_piece) + psqt_map[abs_piece][7-i][7-j])\n\n    return [ [white_material] ,\n            [black_material]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:38.003729Z","iopub.execute_input":"2025-02-01T10:20:38.003956Z","iopub.status.idle":"2025-02-01T10:20:38.020207Z","shell.execute_reply.started":"2025-02-01T10:20:38.003938Z","shell.execute_reply":"2025-02-01T10:20:38.019328Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def encode_additional_features(fen):\n    board = getBoard(fen)\n    features = (pawn_structure_features(board) + pass_pawn_features(board) + material_features(board) + rookFeatures(board) + BishopFeatures(board) + KnightFeatures(board))\n    result = np.concatenate(features)\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:38.020971Z","iopub.execute_input":"2025-02-01T10:20:38.021243Z","iopub.status.idle":"2025-02-01T10:20:38.034902Z","shell.execute_reply.started":"2025-02-01T10:20:38.021222Z","shell.execute_reply":"2025-02-01T10:20:38.034247Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Neural Network Model (CNN)","metadata":{}},{"cell_type":"code","source":"\ndef build_deep_model():\n\n  input = tensorflow.keras.layers.Input(shape = (8,8,14))\n\n  input_additonal = tensorflow.keras.layers.Input(shape = (42,))\n\n  x = tensorflow.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation = 'relu')(input)\n  z = tensorflow.keras.layers.Conv2D(filters=32, kernel_size=(4,4), padding='same', activation='relu')(input)\n  y = tensorflow.keras.layers.Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu')(input)\n\n  for _ in range(4):\n      previous = x\n      x = tensorflow.keras.layers.BatchNormalization()(x)\n      x = tensorflow.keras.layers.Activation('relu')(x)\n      x = tensorflow.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(x)\n      x = tensorflow.keras.layers.BatchNormalization()(x)\n      x = tensorflow.keras.layers.Add()([previous, x])\n      x = tensorflow.keras.layers.Activation('relu')(x)\n      x = tensorflow.keras.layers.Dropout(0.3)(x)\n\n      previous2 = y\n      y = tensorflow.keras.layers.BatchNormalization()(y)\n      y = tensorflow.keras.layers.Activation('relu')(y)\n      y = tensorflow.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), padding='same', activation='relu')(y)\n      y = tensorflow.keras.layers.BatchNormalization()(y)\n      y = tensorflow.keras.layers.Add()([previous2, y])\n      y = tensorflow.keras.layers.Activation('relu')(y)\n      y = tensorflow.keras.layers.Dropout(0.3)(y)\n\n\n      previous3 = z\n      z = tensorflow.keras.layers.BatchNormalization()(z)\n      z = tensorflow.keras.layers.Activation('relu')(z)\n      z = tensorflow.keras.layers.Conv2D(filters=32, kernel_size=(4, 4), padding='same', activation='relu')(z)\n      z = tensorflow.keras.layers.BatchNormalization()(z)\n      z = tensorflow.keras.layers.Add()([previous3,z])\n      z = tensorflow.keras.layers.Activation('relu')(z)\n      z = tensorflow.keras.layers.Dropout(0.3)(z)\n\n\n  x = tensorflow.keras.layers.Flatten()(x)\n  y = tensorflow.keras.layers.Flatten()(y)\n  z = tensorflow.keras.layers.Flatten()(z)\n\n  cnn_features = tensorflow.keras.layers.Add()([x, y, z])\n  x = tensorflow.keras.layers.Concatenate()([cnn_features, input_additonal])\n\n  x = tensorflow.keras.layers.Dense(2090, activation = 'relu')(x)\n  x = tensorflow.keras.layers.Dropout(0.3)(x)\n  x = tensorflow.keras.layers.Dense(1)(x)\n\n  model = tensorflow.keras.Model(inputs = [input, input_additonal], outputs = x)\n\n  model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mae'])\n\n  model.summary()\n\n  return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:38.035659Z","iopub.execute_input":"2025-02-01T10:20:38.035937Z","iopub.status.idle":"2025-02-01T10:20:38.049018Z","shell.execute_reply.started":"2025-02-01T10:20:38.035909Z","shell.execute_reply":"2025-02-01T10:20:38.048155Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Load the traning data","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv(\"/kaggle/input/training-data/Chess_1.csv\")\ndata2 = pd.read_csv(\"/kaggle/input/training-data/Chess_2.csv\")\nchess_data_1 = processData(data1)\nchess_data_2 = processData(data2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:20:38.049880Z","iopub.execute_input":"2025-02-01T10:20:38.050161Z","iopub.status.idle":"2025-02-01T10:21:39.398439Z","shell.execute_reply.started":"2025-02-01T10:20:38.050133Z","shell.execute_reply":"2025-02-01T10:21:39.397330Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# model1 = build_deep_model() # trained on chess1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:21:39.399317Z","iopub.execute_input":"2025-02-01T10:21:39.399535Z","iopub.status.idle":"2025-02-01T10:21:39.402799Z","shell.execute_reply.started":"2025-02-01T10:21:39.399517Z","shell.execute_reply":"2025-02-01T10:21:39.402061Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"model2 = build_deep_model() ## traning on chesss1 and chess2 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:21:39.403642Z","iopub.execute_input":"2025-02-01T10:21:39.403905Z","iopub.status.idle":"2025-02-01T10:21:42.696140Z","shell.execute_reply.started":"2025-02-01T10:21:39.403871Z","shell.execute_reply":"2025-02-01T10:21:42.695169Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m14\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │          \u001b[38;5;34m4,064\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m11,232\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │          \u001b[38;5;34m7,200\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_2 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_4 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_4… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │          \u001b[38;5;34m9,248\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m25,632\u001b[0m │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m16,416\u001b[0m │ activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                           │                        │                │ batch_normalization_3… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_2 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                           │                        │                │ batch_normalization_5… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_1 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_3 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_5 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_6     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_8     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_10    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_6 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_6… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_8 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_8… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_10             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │          \u001b[38;5;34m9,248\u001b[0m │ activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m25,632\u001b[0m │ activation_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m16,416\u001b[0m │ activation_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_9     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_11    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_3 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n│                           │                        │                │ batch_normalization_7… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_4 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ batch_normalization_9… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_5 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_7 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_9 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_11             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_14    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_16    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_12             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_14             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_16             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │          \u001b[38;5;34m9,248\u001b[0m │ activation_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m25,632\u001b[0m │ activation_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m16,416\u001b[0m │ activation_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_15    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_17    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_6 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_7 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_8 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_13             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_15             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_17             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_18    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_20    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_22    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_18             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_20             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_22             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │          \u001b[38;5;34m9,248\u001b[0m │ activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m25,632\u001b[0m │ activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m16,416\u001b[0m │ activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_19    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_21    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_23    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_9 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_10 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_11 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_19             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_21             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_23             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_12 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n│                           │                        │                │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2090\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n│                           │                        │                │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2090\u001b[0m)           │      \u001b[38;5;34m4,370,190\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2090\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │          \u001b[38;5;34m2,091\u001b[0m │ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,064</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">11,232</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">7,200</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_4… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">25,632</span> │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,416</span> │ activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                           │                        │                │ batch_normalization_3… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                           │                        │                │ batch_normalization_5… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_6     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_8     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_10    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_6… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_8… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_10             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">25,632</span> │ activation_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,416</span> │ activation_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_9     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_11    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n│                           │                        │                │ batch_normalization_7… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ batch_normalization_9… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_11             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_14    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_16    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_12             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_14             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_16             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">25,632</span> │ activation_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,416</span> │ activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_15    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_17    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_13             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_15             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_17             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_18    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_20    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_22    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_18             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_20             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_22             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">25,632</span> │ activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,416</span> │ activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_19    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_21    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_23    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ batch_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_19             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_21             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_23             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n│                           │                        │                │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2090</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n│                           │                        │                │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2090</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,370,190</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2090</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,091</span> │ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,603,033\u001b[0m (17.56 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,603,033</span> (17.56 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,601,497\u001b[0m (17.55 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,601,497</span> (17.55 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n</pre>\n"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# training on chess1.csv and chess2.csv\n\ndata_splits = np.array_split(chess_data_2, 10)\n\nfor epoch in range(3):\n    for i,data_chunk in enumerate(data_splits):\n\n      train_data = data_chunk\n      print(f\"encoding chunk {i+1}/{len(data_splits)}..  (shape = {train_data.shape[0]})\")\n\n      y_train = train_data['Evaluation'].values\n      encoded_fens_train = train_data['FEN'].apply(encoding8814)\n      x_train = np.stack(encoded_fens_train.to_list())\n      x_additional = np.stack(train_data['FEN'].apply(encode_additional_features).to_list())\n\n      print(\"encoded!\")\n      print(\"training model...\")\n      # model1.fit(x_train, y_train, batch_size = 1024, validation_split=0.2, epochs = 10, verbose = 1)\n      model2.fit([x_train, x_additional], y_train, batch_size=1024, validation_split=0.2, epochs=10, verbose=1)\n\nprint(\"trained on chess 2!\")\n\n\ndata_splits = np.array_split(chess_data_1, 10)\n\nfor epoch in range(3):\n    for i,data_chunk in enumerate(data_splits):\n\n      train_data = data_chunk\n      print(f\"encoding chunk {i+1}/{len(data_splits)}..  (shape = {train_data.shape[0]})\")\n\n      y_train = train_data['Evaluation'].values\n      encoded_fens_train = train_data['FEN'].apply(encoding8814)\n      x_train = np.stack(encoded_fens_train.to_list())\n      x_additional = np.stack(train_data['FEN'].apply(encode_additional_features).to_list())\n\n      print(\"encoded!\")\n      print(\"training model...\")\n      # model1.fit(x_train, y_train, batch_size = 1024, validation_split=0.2, epochs = 10, verbose = 1)\n      model2.fit([x_train, x_additional], y_train, batch_size=1024, validation_split=0.2, epochs=10, verbose=1)\n\nprint(\"trained on chess 1!\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:21:42.697023Z","iopub.execute_input":"2025-02-01T10:21:42.697364Z","execution_failed":"2025-02-01T14:46:00.798Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"},{"name":"stdout","text":"encoding chunk 1/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 178ms/step - loss: 178133.3281 - mae: 244.7719 - val_loss: 209907.6875 - val_mae: 231.4301\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - loss: 80869.4219 - mae: 171.6637 - val_loss: 203834.2812 - val_mae: 248.7146\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - loss: 71030.4297 - mae: 159.9419 - val_loss: 133284.8438 - val_mae: 223.0594\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - loss: 61896.4023 - mae: 149.8292 - val_loss: 102365.8594 - val_mae: 211.3282\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - loss: 56438.9961 - mae: 143.6978 - val_loss: 75098.0547 - val_mae: 165.7335\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - loss: 50278.4922 - mae: 137.4396 - val_loss: 75491.3516 - val_mae: 160.3220\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - loss: 45395.6328 - mae: 132.6804 - val_loss: 66750.5781 - val_mae: 145.7946\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 43353.0312 - mae: 131.3812 - val_loss: 80492.7109 - val_mae: 145.9572\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 39584.1172 - mae: 127.6379 - val_loss: 64947.8477 - val_mae: 143.1264\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 38252.7344 - mae: 126.0538 - val_loss: 66718.8828 - val_mae: 141.7014\nencoding chunk 2/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 66783.4844 - mae: 143.4201 - val_loss: 68160.3594 - val_mae: 153.4649\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - loss: 57020.0469 - mae: 138.7807 - val_loss: 60361.7383 - val_mae: 137.3970\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 49860.1016 - mae: 133.0355 - val_loss: 62820.7500 - val_mae: 138.1149\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 45101.0273 - mae: 129.9503 - val_loss: 62419.7031 - val_mae: 137.3410\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 42062.5234 - mae: 126.8606 - val_loss: 66217.5781 - val_mae: 152.3078\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 39137.2305 - mae: 125.3487 - val_loss: 64360.4922 - val_mae: 139.8176\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 36845.5977 - mae: 123.8766 - val_loss: 60567.1641 - val_mae: 136.7894\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 34711.3516 - mae: 121.1112 - val_loss: 59264.9570 - val_mae: 133.5413\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 34023.2930 - mae: 121.1535 - val_loss: 62267.6406 - val_mae: 140.5725\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 33764.5430 - mae: 120.4132 - val_loss: 64791.6367 - val_mae: 140.5093\nencoding chunk 3/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 64189.2383 - mae: 139.8966 - val_loss: 50675.4102 - val_mae: 127.0861\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 51329.3750 - mae: 131.9858 - val_loss: 51791.6914 - val_mae: 130.8016\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 44772.7266 - mae: 128.4581 - val_loss: 54694.6875 - val_mae: 127.5666\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 42328.5117 - mae: 126.1379 - val_loss: 58171.2930 - val_mae: 129.9534\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 39331.2969 - mae: 124.0423 - val_loss: 53281.5312 - val_mae: 126.6815\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 37294.7148 - mae: 122.1700 - val_loss: 55197.7930 - val_mae: 129.9149\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 35911.9922 - mae: 121.4411 - val_loss: 55926.0977 - val_mae: 130.8617\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 34658.5586 - mae: 119.7465 - val_loss: 58250.5156 - val_mae: 129.4641\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 33163.9102 - mae: 119.2776 - val_loss: 56120.3906 - val_mae: 126.3241\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 31886.9180 - mae: 117.6007 - val_loss: 66377.0859 - val_mae: 151.8716\nencoding chunk 4/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 59127.8438 - mae: 135.3044 - val_loss: 55064.2617 - val_mae: 136.9147\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 48936.0625 - mae: 130.3109 - val_loss: 54216.0508 - val_mae: 129.9115\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 43418.2969 - mae: 126.5827 - val_loss: 56710.6172 - val_mae: 136.7715\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 39211.4414 - mae: 122.8088 - val_loss: 56972.9648 - val_mae: 135.0249\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 37941.5000 - mae: 122.4267 - val_loss: 51531.8320 - val_mae: 124.8804\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 35189.0703 - mae: 120.2086 - val_loss: 53090.4102 - val_mae: 124.2231\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 33133.6680 - mae: 118.5094 - val_loss: 52943.2031 - val_mae: 128.9634\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 32645.4590 - mae: 117.7968 - val_loss: 62636.1094 - val_mae: 150.8129\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 31979.5879 - mae: 117.5561 - val_loss: 53581.1055 - val_mae: 124.1178\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30696.6113 - mae: 116.5251 - val_loss: 53857.1562 - val_mae: 123.7561\nencoding chunk 5/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 56237.3672 - mae: 132.2425 - val_loss: 51226.8008 - val_mae: 127.2220\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 45469.4102 - mae: 126.1511 - val_loss: 51262.3633 - val_mae: 127.1106\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 41534.3594 - mae: 123.1950 - val_loss: 53214.3594 - val_mae: 127.8240\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 39438.3359 - mae: 122.5368 - val_loss: 55847.6758 - val_mae: 133.2438\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 35829.1523 - mae: 120.0143 - val_loss: 56949.0078 - val_mae: 131.3536\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 34509.4062 - mae: 118.4561 - val_loss: 52840.3438 - val_mae: 123.8894\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 32536.4121 - mae: 117.4072 - val_loss: 55089.0625 - val_mae: 128.2332\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 32328.7461 - mae: 117.7999 - val_loss: 51938.4648 - val_mae: 125.8112\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30804.8809 - mae: 115.2905 - val_loss: 56523.1328 - val_mae: 122.4691\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30024.1777 - mae: 114.3330 - val_loss: 53548.5469 - val_mae: 127.0735\nencoding chunk 6/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 55622.9570 - mae: 130.7285 - val_loss: 48889.2188 - val_mae: 122.5687\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 45216.3516 - mae: 125.3962 - val_loss: 50580.6992 - val_mae: 123.2061\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 41492.6992 - mae: 123.1080 - val_loss: 48430.6758 - val_mae: 120.0313\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 37461.5508 - mae: 119.9662 - val_loss: 50373.9492 - val_mae: 121.0649\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 35544.2148 - mae: 119.4508 - val_loss: 50786.9062 - val_mae: 121.3201\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 34119.1836 - mae: 116.8848 - val_loss: 50596.2461 - val_mae: 123.2182\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 33012.3164 - mae: 117.5755 - val_loss: 50718.7109 - val_mae: 125.0029\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 31685.1777 - mae: 115.6416 - val_loss: 52264.0430 - val_mae: 122.3835\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 31065.8203 - mae: 114.9636 - val_loss: 49266.2930 - val_mae: 120.5401\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30124.9668 - mae: 113.7782 - val_loss: 51249.6055 - val_mae: 119.5209\nencoding chunk 7/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 52540.5156 - mae: 128.9028 - val_loss: 48277.3398 - val_mae: 125.2202\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 44112.9844 - mae: 125.5601 - val_loss: 46851.9883 - val_mae: 122.0407\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 38946.4922 - mae: 121.2118 - val_loss: 46093.1680 - val_mae: 119.1239\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 35392.6328 - mae: 119.1406 - val_loss: 48236.3672 - val_mae: 121.7740\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 33216.6250 - mae: 116.6540 - val_loss: 46900.0586 - val_mae: 118.6765\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 32566.9121 - mae: 116.2554 - val_loss: 49250.2812 - val_mae: 125.9956\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30098.0020 - mae: 113.9945 - val_loss: 48323.3906 - val_mae: 121.7721\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30006.7617 - mae: 113.6376 - val_loss: 50031.0430 - val_mae: 121.1528\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29559.8340 - mae: 112.8403 - val_loss: 47782.4336 - val_mae: 118.2975\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 29061.7148 - mae: 112.9905 - val_loss: 47470.4727 - val_mae: 120.3605\nencoding chunk 8/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 52225.3203 - mae: 128.3279 - val_loss: 47958.8789 - val_mae: 122.7647\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 42276.6484 - mae: 123.0746 - val_loss: 46246.6445 - val_mae: 118.9754\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 39384.9492 - mae: 121.2756 - val_loss: 51976.9375 - val_mae: 129.6031\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 36094.5586 - mae: 119.1205 - val_loss: 52194.8125 - val_mae: 128.4175\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 33404.9609 - mae: 116.4655 - val_loss: 49582.4141 - val_mae: 118.0779\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 32032.3379 - mae: 115.6796 - val_loss: 50613.3594 - val_mae: 120.2518\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 31545.6113 - mae: 114.8890 - val_loss: 50877.1875 - val_mae: 119.5757\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30566.6641 - mae: 114.3924 - val_loss: 56025.1562 - val_mae: 133.0282\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29633.7480 - mae: 113.1331 - val_loss: 51758.7266 - val_mae: 122.9873\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29887.6484 - mae: 113.0358 - val_loss: 53628.8984 - val_mae: 128.3078\nencoding chunk 9/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 53540.0547 - mae: 127.6345 - val_loss: 43942.1211 - val_mae: 121.8299\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 42660.6328 - mae: 122.3761 - val_loss: 43590.4102 - val_mae: 117.2363\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 38026.2773 - mae: 119.3705 - val_loss: 47112.8125 - val_mae: 119.3210\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 36074.8789 - mae: 117.8465 - val_loss: 46406.4062 - val_mae: 121.4719\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 34904.4492 - mae: 117.1987 - val_loss: 46629.9922 - val_mae: 120.8502\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 32541.3242 - mae: 115.2395 - val_loss: 51178.7695 - val_mae: 130.0712\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 31495.0352 - mae: 114.0200 - val_loss: 48027.6602 - val_mae: 122.1461\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30606.2812 - mae: 113.2190 - val_loss: 47459.5469 - val_mae: 117.1172\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29788.4492 - mae: 112.6894 - val_loss: 48001.6562 - val_mae: 119.4224\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27835.6133 - mae: 110.3229 - val_loss: 46689.0000 - val_mae: 116.6423\nencoding chunk 10/10..  (shape = 198049)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 158ms/step - loss: 53117.7461 - mae: 126.2882 - val_loss: 49644.9336 - val_mae: 122.6063\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - loss: 42817.3242 - mae: 121.9450 - val_loss: 55016.2734 - val_mae: 135.9125\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 39372.5352 - mae: 119.8818 - val_loss: 50064.6367 - val_mae: 121.2016\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 34889.5781 - mae: 116.4487 - val_loss: 54988.1680 - val_mae: 131.7555\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 33935.6445 - mae: 116.5059 - val_loss: 51954.8633 - val_mae: 120.4482\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 32106.3711 - mae: 114.0460 - val_loss: 51686.4336 - val_mae: 120.7867\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 31032.7695 - mae: 113.5678 - val_loss: 51256.9414 - val_mae: 118.1943\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30481.6738 - mae: 113.3050 - val_loss: 50909.3633 - val_mae: 120.9717\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 29035.9199 - mae: 111.9879 - val_loss: 52058.4102 - val_mae: 118.3998\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28175.0176 - mae: 110.9171 - val_loss: 52078.6094 - val_mae: 118.6327\nencoding chunk 1/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 46863.6172 - mae: 121.4787 - val_loss: 45594.4453 - val_mae: 118.4417\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 38997.2227 - mae: 118.5027 - val_loss: 48016.6953 - val_mae: 118.9949\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 36775.1406 - mae: 116.3672 - val_loss: 47840.1406 - val_mae: 118.2727\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 32931.2773 - mae: 114.0528 - val_loss: 49852.1172 - val_mae: 119.0490\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 31119.9141 - mae: 112.5551 - val_loss: 48611.5430 - val_mae: 117.7050\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28622.6660 - mae: 109.8847 - val_loss: 52084.7852 - val_mae: 125.5304\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29115.9688 - mae: 111.1417 - val_loss: 49360.4180 - val_mae: 117.9378\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27973.0547 - mae: 109.7745 - val_loss: 48267.7500 - val_mae: 115.4831\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27143.5898 - mae: 109.1926 - val_loss: 48269.1055 - val_mae: 115.4802\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26363.6309 - mae: 108.5489 - val_loss: 47082.1680 - val_mae: 115.3669\nencoding chunk 2/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 48332.8242 - mae: 122.6957 - val_loss: 45767.1016 - val_mae: 118.6087\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 39734.8516 - mae: 119.4651 - val_loss: 43512.9805 - val_mae: 117.1384\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 35557.3906 - mae: 116.4758 - val_loss: 46397.3945 - val_mae: 121.3464\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 32107.2461 - mae: 114.3818 - val_loss: 44866.7734 - val_mae: 115.7841\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 31612.6855 - mae: 113.7664 - val_loss: 49781.0664 - val_mae: 132.7198\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30429.2480 - mae: 113.1099 - val_loss: 49690.2266 - val_mae: 124.8532\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28970.3984 - mae: 111.3739 - val_loss: 46621.9219 - val_mae: 119.2837\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28720.9766 - mae: 110.3766 - val_loss: 45943.6133 - val_mae: 116.9460\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27842.6699 - mae: 109.8688 - val_loss: 46139.1250 - val_mae: 114.7846\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27505.6836 - mae: 109.4002 - val_loss: 45826.2031 - val_mae: 115.0401\nencoding chunk 3/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 50298.3008 - mae: 123.4602 - val_loss: 43991.3789 - val_mae: 117.3109\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 38947.4375 - mae: 117.6783 - val_loss: 44579.1602 - val_mae: 116.3364\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 36128.6641 - mae: 116.1989 - val_loss: 45599.0039 - val_mae: 120.4112\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 32645.8379 - mae: 114.0460 - val_loss: 44036.8750 - val_mae: 116.0872\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 32560.5156 - mae: 113.5599 - val_loss: 45175.2656 - val_mae: 114.4807\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30120.0156 - mae: 112.1585 - val_loss: 45159.0742 - val_mae: 115.5924\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 30216.8242 - mae: 110.7120 - val_loss: 44435.6836 - val_mae: 113.8982\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29768.9863 - mae: 110.3611 - val_loss: 45465.4219 - val_mae: 114.1694\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28830.0527 - mae: 110.0176 - val_loss: 44029.5586 - val_mae: 112.3855\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26755.4785 - mae: 108.2212 - val_loss: 47745.6680 - val_mae: 115.4315\nencoding chunk 4/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 46866.5547 - mae: 121.3897 - val_loss: 51006.2617 - val_mae: 124.5838\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 39511.9766 - mae: 118.4642 - val_loss: 44623.0703 - val_mae: 116.4711\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 37018.5000 - mae: 117.2020 - val_loss: 44952.9688 - val_mae: 113.5834\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 31590.9434 - mae: 113.4549 - val_loss: 44941.4961 - val_mae: 113.3763\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30444.6641 - mae: 112.3973 - val_loss: 46415.1055 - val_mae: 117.8601\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29117.7480 - mae: 110.7359 - val_loss: 47478.9922 - val_mae: 117.9579\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28376.4590 - mae: 109.9581 - val_loss: 46401.4492 - val_mae: 115.0279\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28100.6211 - mae: 109.6204 - val_loss: 45538.5078 - val_mae: 113.9268\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27455.4043 - mae: 108.9407 - val_loss: 47420.0352 - val_mae: 115.3447\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27057.8223 - mae: 108.2529 - val_loss: 48025.2539 - val_mae: 115.1600\nencoding chunk 5/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 45489.7344 - mae: 121.2193 - val_loss: 47352.6094 - val_mae: 117.8147\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 38091.4258 - mae: 117.0209 - val_loss: 44282.7109 - val_mae: 114.8249\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 34854.5078 - mae: 114.7265 - val_loss: 49150.2852 - val_mae: 123.6312\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 33354.5195 - mae: 114.0920 - val_loss: 44499.4180 - val_mae: 113.4365\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 30667.5566 - mae: 111.7433 - val_loss: 46958.4609 - val_mae: 115.0725\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 29690.2012 - mae: 110.2316 - val_loss: 47457.7891 - val_mae: 116.3180\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28453.9453 - mae: 109.3434 - val_loss: 46769.8477 - val_mae: 118.6828\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27117.6250 - mae: 108.6031 - val_loss: 45537.8086 - val_mae: 114.8835\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27107.7598 - mae: 108.7299 - val_loss: 47966.6523 - val_mae: 116.7237\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26278.0215 - mae: 107.2291 - val_loss: 47427.4688 - val_mae: 114.0760\nencoding chunk 6/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 45942.3750 - mae: 119.8115 - val_loss: 47485.7578 - val_mae: 123.4740\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - loss: 37850.2461 - mae: 116.8096 - val_loss: 43176.7930 - val_mae: 116.3861\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 34263.7383 - mae: 114.6628 - val_loss: 44362.5391 - val_mae: 117.2591\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 32522.3340 - mae: 112.9386 - val_loss: 48481.7539 - val_mae: 125.7187\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29706.4629 - mae: 111.0033 - val_loss: 43446.5703 - val_mae: 114.5814\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28683.5469 - mae: 110.2210 - val_loss: 43803.0586 - val_mae: 113.5576\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27854.2871 - mae: 109.5628 - val_loss: 46562.5273 - val_mae: 120.0137\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27363.8711 - mae: 108.1711 - val_loss: 46581.5039 - val_mae: 117.8479\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26437.9199 - mae: 108.1953 - val_loss: 47447.3906 - val_mae: 120.6464\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26227.8262 - mae: 107.4314 - val_loss: 45152.4609 - val_mae: 110.7508\nencoding chunk 7/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 47132.7031 - mae: 121.1595 - val_loss: 42010.1523 - val_mae: 112.5507\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 36384.4883 - mae: 116.4619 - val_loss: 44549.1250 - val_mae: 118.2098\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 33410.2500 - mae: 113.7550 - val_loss: 44357.6172 - val_mae: 114.0027\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 30449.9355 - mae: 111.3733 - val_loss: 42794.1836 - val_mae: 111.8072\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30211.3359 - mae: 111.0823 - val_loss: 43653.2852 - val_mae: 111.9368\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28518.0059 - mae: 109.2968 - val_loss: 45034.0547 - val_mae: 112.0562\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27307.7695 - mae: 108.0473 - val_loss: 44270.8984 - val_mae: 112.8083\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26841.3125 - mae: 107.9005 - val_loss: 43990.3711 - val_mae: 112.5177\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26001.6465 - mae: 107.5968 - val_loss: 48191.8945 - val_mae: 128.1892\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25898.3477 - mae: 107.6550 - val_loss: 44233.1250 - val_mae: 111.6307\nencoding chunk 8/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 44935.8164 - mae: 120.3062 - val_loss: 46551.5039 - val_mae: 118.2522\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 38479.9141 - mae: 116.9289 - val_loss: 45517.2617 - val_mae: 114.0746\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 34103.5039 - mae: 113.6754 - val_loss: 47120.1719 - val_mae: 112.5796\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 31368.9414 - mae: 111.9036 - val_loss: 49283.8086 - val_mae: 124.2556\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 31205.6738 - mae: 111.3924 - val_loss: 47036.8203 - val_mae: 112.4954\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 29422.7734 - mae: 110.0666 - val_loss: 48348.7695 - val_mae: 122.3704\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28263.5781 - mae: 109.3559 - val_loss: 45544.0078 - val_mae: 111.1680\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27958.8047 - mae: 108.5982 - val_loss: 46760.8867 - val_mae: 110.4668\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27631.1543 - mae: 108.1193 - val_loss: 48828.5117 - val_mae: 122.4927\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27295.8086 - mae: 108.2199 - val_loss: 49525.7773 - val_mae: 115.2841\nencoding chunk 9/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 46096.1641 - mae: 119.9361 - val_loss: 40636.0859 - val_mae: 114.3612\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 38095.6211 - mae: 116.0304 - val_loss: 42343.8008 - val_mae: 112.3752\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 35002.8320 - mae: 114.1158 - val_loss: 40716.0391 - val_mae: 113.5024\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 31535.3223 - mae: 111.8144 - val_loss: 43349.6562 - val_mae: 111.6815\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30993.0254 - mae: 110.3684 - val_loss: 41334.0781 - val_mae: 110.4474\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29260.1094 - mae: 109.9355 - val_loss: 42930.2500 - val_mae: 110.9037\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27214.4648 - mae: 108.1181 - val_loss: 42976.8438 - val_mae: 113.8364\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27322.2578 - mae: 109.0725 - val_loss: 43031.8555 - val_mae: 111.6594\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26593.9082 - mae: 106.7659 - val_loss: 44593.9844 - val_mae: 111.3396\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26921.8848 - mae: 107.3111 - val_loss: 42663.7852 - val_mae: 110.8350\nencoding chunk 10/10..  (shape = 198049)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 43237.3594 - mae: 118.5298 - val_loss: 46263.5586 - val_mae: 117.1887\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 38360.2891 - mae: 116.7077 - val_loss: 44947.9180 - val_mae: 113.3548\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 34465.3203 - mae: 113.4065 - val_loss: 46372.6719 - val_mae: 114.0596\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 31825.5234 - mae: 110.9994 - val_loss: 48549.4570 - val_mae: 115.7311\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30914.0703 - mae: 111.4379 - val_loss: 50534.8945 - val_mae: 119.2091\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28515.6523 - mae: 108.6761 - val_loss: 46651.4258 - val_mae: 113.1805\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 29188.1875 - mae: 108.9946 - val_loss: 49722.8633 - val_mae: 120.5029\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27197.4902 - mae: 107.5314 - val_loss: 47762.6445 - val_mae: 117.3622\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26889.1934 - mae: 107.6975 - val_loss: 49739.1797 - val_mae: 113.2396\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26475.6152 - mae: 107.2896 - val_loss: 47152.5469 - val_mae: 112.1413\nencoding chunk 1/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 42424.7422 - mae: 116.4126 - val_loss: 41801.6250 - val_mae: 113.4905\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 34158.9180 - mae: 112.0659 - val_loss: 43532.1250 - val_mae: 115.6156\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31613.6738 - mae: 110.6504 - val_loss: 44483.8672 - val_mae: 115.8395\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 29882.3301 - mae: 109.8101 - val_loss: 43278.7500 - val_mae: 112.0657\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27824.8164 - mae: 107.5696 - val_loss: 47311.7812 - val_mae: 114.4659\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26511.8477 - mae: 106.5058 - val_loss: 45968.3672 - val_mae: 111.3577\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26257.1445 - mae: 106.4158 - val_loss: 43446.3203 - val_mae: 112.5429\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26274.7305 - mae: 106.0090 - val_loss: 43800.6289 - val_mae: 111.2544\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25024.5469 - mae: 104.5885 - val_loss: 44402.9727 - val_mae: 111.3280\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 24606.1855 - mae: 103.9298 - val_loss: 43571.0273 - val_mae: 112.0147\nencoding chunk 2/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 42876.8242 - mae: 116.1514 - val_loss: 44538.5664 - val_mae: 119.6224\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 37399.1914 - mae: 115.0642 - val_loss: 41638.5117 - val_mae: 112.2818\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 32368.9961 - mae: 111.9533 - val_loss: 43260.9453 - val_mae: 112.2033\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 30007.7324 - mae: 110.2130 - val_loss: 41662.3633 - val_mae: 110.3763\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29061.2324 - mae: 109.3085 - val_loss: 44770.6484 - val_mae: 112.3027\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28224.1914 - mae: 108.5514 - val_loss: 42307.9961 - val_mae: 111.2646\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25970.9707 - mae: 106.5148 - val_loss: 42695.1758 - val_mae: 109.7857\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26544.4766 - mae: 106.8813 - val_loss: 42255.5781 - val_mae: 111.9391\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25555.6348 - mae: 105.5763 - val_loss: 42491.6641 - val_mae: 109.7153\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25140.0508 - mae: 105.0533 - val_loss: 43103.4023 - val_mae: 113.0414\nencoding chunk 3/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 44214.1250 - mae: 117.2964 - val_loss: 44656.8516 - val_mae: 114.2787\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 36253.3203 - mae: 114.5390 - val_loss: 41318.6211 - val_mae: 110.2856\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 32990.5234 - mae: 112.0501 - val_loss: 41575.3789 - val_mae: 112.9042\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 30474.8105 - mae: 111.0005 - val_loss: 40716.3750 - val_mae: 108.0496\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28499.8398 - mae: 107.2635 - val_loss: 42727.8711 - val_mae: 113.9124\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27650.9746 - mae: 107.6135 - val_loss: 44028.5195 - val_mae: 110.9850\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27058.5723 - mae: 107.4283 - val_loss: 43399.6406 - val_mae: 111.2307\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26188.8691 - mae: 106.3458 - val_loss: 43942.2188 - val_mae: 110.4602\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26123.4434 - mae: 105.9514 - val_loss: 42819.0977 - val_mae: 110.2662\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25951.9980 - mae: 105.5572 - val_loss: 42835.8164 - val_mae: 109.7916\nencoding chunk 4/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 43719.6367 - mae: 117.2860 - val_loss: 42117.1836 - val_mae: 112.0756\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 35012.9258 - mae: 113.8394 - val_loss: 44466.8008 - val_mae: 115.2672\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 32080.7500 - mae: 111.9055 - val_loss: 44356.8867 - val_mae: 117.7029\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 29992.0625 - mae: 110.3845 - val_loss: 44556.2773 - val_mae: 111.2186\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28755.7988 - mae: 108.7406 - val_loss: 44096.3125 - val_mae: 111.2133\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27392.8867 - mae: 107.4611 - val_loss: 44662.2812 - val_mae: 114.0664\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25869.8008 - mae: 106.0954 - val_loss: 43120.7617 - val_mae: 110.2260\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26190.6895 - mae: 106.4672 - val_loss: 44205.8164 - val_mae: 113.7401\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25766.4883 - mae: 106.3514 - val_loss: 43169.9766 - val_mae: 109.1729\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25450.9355 - mae: 105.1158 - val_loss: 43807.1562 - val_mae: 108.9830\nencoding chunk 5/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 43709.5391 - mae: 118.5641 - val_loss: 43589.2461 - val_mae: 114.3437\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 35586.6094 - mae: 113.8522 - val_loss: 45274.7344 - val_mae: 113.1620\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30469.1543 - mae: 109.9400 - val_loss: 44809.4766 - val_mae: 113.0012\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 29559.4473 - mae: 109.2612 - val_loss: 43240.3711 - val_mae: 109.5447\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28322.0312 - mae: 107.9633 - val_loss: 45632.0586 - val_mae: 111.5687\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26825.8301 - mae: 106.9017 - val_loss: 44519.7031 - val_mae: 109.1473\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26800.2402 - mae: 106.9695 - val_loss: 43979.8789 - val_mae: 109.9274\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26256.0332 - mae: 106.1777 - val_loss: 44070.9336 - val_mae: 110.3524\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25774.8691 - mae: 105.2594 - val_loss: 43878.1055 - val_mae: 109.3302\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25537.9980 - mae: 105.3386 - val_loss: 45267.7578 - val_mae: 111.6508\nencoding chunk 6/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 40964.4766 - mae: 115.4311 - val_loss: 41486.0195 - val_mae: 113.3703\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 36052.2461 - mae: 113.8362 - val_loss: 42098.3984 - val_mae: 110.7104\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30979.6719 - mae: 110.0599 - val_loss: 41848.3711 - val_mae: 109.1627\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 29534.8613 - mae: 109.2325 - val_loss: 42817.6328 - val_mae: 109.5203\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27724.3418 - mae: 107.6507 - val_loss: 42318.0664 - val_mae: 109.0823\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27324.1680 - mae: 108.0428 - val_loss: 42278.6211 - val_mae: 108.9479\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25704.7832 - mae: 105.3317 - val_loss: 42882.1992 - val_mae: 110.3045\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26408.0098 - mae: 106.0624 - val_loss: 42929.6992 - val_mae: 108.5886\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25417.2539 - mae: 105.8199 - val_loss: 42037.5859 - val_mae: 108.7465\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25008.5918 - mae: 104.6293 - val_loss: 45332.2305 - val_mae: 115.5114\nencoding chunk 7/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - loss: 40350.6250 - mae: 115.2935 - val_loss: 40665.4141 - val_mae: 110.0713\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 34434.6250 - mae: 113.2343 - val_loss: 41293.4531 - val_mae: 111.7130\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31073.6270 - mae: 110.7795 - val_loss: 40417.2148 - val_mae: 109.5955\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 28554.2227 - mae: 108.4837 - val_loss: 42886.4141 - val_mae: 109.2573\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27494.9180 - mae: 107.3075 - val_loss: 43840.1172 - val_mae: 108.4589\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25743.7559 - mae: 105.5880 - val_loss: 41158.8555 - val_mae: 107.7449\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25734.5215 - mae: 105.5845 - val_loss: 43107.0312 - val_mae: 110.3060\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 24935.8262 - mae: 104.2905 - val_loss: 44515.8164 - val_mae: 112.3636\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25400.3633 - mae: 105.0839 - val_loss: 42807.7227 - val_mae: 109.0277\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 24438.6348 - mae: 104.2594 - val_loss: 42794.6055 - val_mae: 109.1127\nencoding chunk 8/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 40886.1445 - mae: 115.5814 - val_loss: 42934.5039 - val_mae: 110.9380\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 34420.4062 - mae: 112.8902 - val_loss: 46255.3984 - val_mae: 116.2499\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30851.5625 - mae: 109.7683 - val_loss: 43374.8164 - val_mae: 109.1242\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 29534.9102 - mae: 108.9990 - val_loss: 43960.9336 - val_mae: 108.2492\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28545.4766 - mae: 108.2457 - val_loss: 45992.7734 - val_mae: 110.0574\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27749.3164 - mae: 106.9762 - val_loss: 44005.4805 - val_mae: 109.3856\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26866.3594 - mae: 106.8701 - val_loss: 48071.6055 - val_mae: 113.8237\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25995.4648 - mae: 105.9736 - val_loss: 46151.4883 - val_mae: 109.4813\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25096.4492 - mae: 104.5110 - val_loss: 44348.4766 - val_mae: 107.9065\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 24676.2773 - mae: 104.4220 - val_loss: 45517.8516 - val_mae: 110.7221\nencoding chunk 9/10..  (shape = 198050)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 41292.9219 - mae: 115.6698 - val_loss: 42119.8594 - val_mae: 125.6063\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 35348.7148 - mae: 112.8933 - val_loss: 42093.1875 - val_mae: 110.4044\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 30596.6855 - mae: 110.4214 - val_loss: 40808.7773 - val_mae: 108.8936\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 29026.0059 - mae: 107.9756 - val_loss: 40314.0352 - val_mae: 110.7422\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27300.7148 - mae: 107.1797 - val_loss: 42474.7383 - val_mae: 109.8398\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27770.5234 - mae: 107.0020 - val_loss: 43592.4531 - val_mae: 112.4229\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26268.3477 - mae: 106.3626 - val_loss: 42331.6250 - val_mae: 109.4468\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25167.2051 - mae: 104.4683 - val_loss: 40536.1445 - val_mae: 107.5832\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 24405.9102 - mae: 104.3129 - val_loss: 42015.7578 - val_mae: 109.5392\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 24721.2363 - mae: 104.8204 - val_loss: 42570.4219 - val_mae: 109.1932\nencoding chunk 10/10..  (shape = 198049)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 42112.4727 - mae: 115.9400 - val_loss: 44561.5352 - val_mae: 114.9013\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 33653.7188 - mae: 111.9012 - val_loss: 45361.5117 - val_mae: 114.1479\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31272.5820 - mae: 110.1583 - val_loss: 48062.4219 - val_mae: 113.3514\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 28818.0625 - mae: 108.5526 - val_loss: 44839.4258 - val_mae: 109.9614\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28208.5293 - mae: 107.4669 - val_loss: 45690.5820 - val_mae: 110.6291\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26956.3574 - mae: 106.9171 - val_loss: 46600.4922 - val_mae: 112.3154\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27046.3906 - mae: 106.6321 - val_loss: 46284.4258 - val_mae: 111.5153\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 25904.2363 - mae: 105.2756 - val_loss: 46565.5742 - val_mae: 110.4782\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25450.4531 - mae: 104.3413 - val_loss: 47302.6641 - val_mae: 109.8532\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 24820.1992 - mae: 104.6609 - val_loss: 45858.1328 - val_mae: 109.5651\ntrained on chess 2!\nencoding chunk 1/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 173ms/step - loss: 48419.1484 - mae: 120.0675 - val_loss: 42958.7578 - val_mae: 114.4119\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 38721.8633 - mae: 115.4492 - val_loss: 44780.8672 - val_mae: 118.9977\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 34461.1797 - mae: 112.5042 - val_loss: 43328.0898 - val_mae: 110.6580\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 33678.9648 - mae: 111.5715 - val_loss: 43326.2109 - val_mae: 111.5913\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30931.3828 - mae: 109.6406 - val_loss: 43047.6953 - val_mae: 109.1411\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28974.2383 - mae: 108.5347 - val_loss: 44241.8320 - val_mae: 111.9644\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28000.3320 - mae: 107.6544 - val_loss: 45005.4688 - val_mae: 114.3463\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28398.9902 - mae: 107.5303 - val_loss: 43169.9102 - val_mae: 109.0473\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 26442.1309 - mae: 105.8696 - val_loss: 43048.2344 - val_mae: 111.8139\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27809.2070 - mae: 106.9950 - val_loss: 44090.8242 - val_mae: 110.2119\nencoding chunk 2/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 47476.9688 - mae: 119.0592 - val_loss: 43322.6211 - val_mae: 113.9319\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 40112.7578 - mae: 115.0342 - val_loss: 41613.0664 - val_mae: 113.3798\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 35943.4766 - mae: 112.7114 - val_loss: 41409.8594 - val_mae: 109.3188\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 33900.9102 - mae: 112.1237 - val_loss: 42179.9961 - val_mae: 109.2146\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 31108.6543 - mae: 109.6280 - val_loss: 43269.9609 - val_mae: 110.7325\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 30847.5469 - mae: 109.2941 - val_loss: 42636.7695 - val_mae: 110.4478\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 29885.5605 - mae: 109.1636 - val_loss: 42238.7812 - val_mae: 110.2340\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28448.6621 - mae: 107.7108 - val_loss: 43995.6641 - val_mae: 110.4067\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27396.1855 - mae: 106.8106 - val_loss: 44998.8047 - val_mae: 114.4330\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28084.8535 - mae: 107.1517 - val_loss: 43670.7734 - val_mae: 109.6604\nencoding chunk 3/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - loss: 44448.0234 - mae: 117.2032 - val_loss: 39907.5820 - val_mae: 111.6612\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 38306.0039 - mae: 115.2817 - val_loss: 37775.1445 - val_mae: 109.4841\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 35108.8711 - mae: 112.6997 - val_loss: 40382.9805 - val_mae: 112.6136\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 31553.8379 - mae: 110.6153 - val_loss: 40335.5156 - val_mae: 112.1880\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 31170.0586 - mae: 110.0238 - val_loss: 39370.7773 - val_mae: 109.1758\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 29611.7910 - mae: 108.5826 - val_loss: 42229.4297 - val_mae: 113.1022\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28767.3164 - mae: 107.3906 - val_loss: 39778.2344 - val_mae: 108.7487\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28205.2578 - mae: 107.4952 - val_loss: 40952.6445 - val_mae: 112.9986\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 27684.4707 - mae: 106.7055 - val_loss: 42135.0156 - val_mae: 116.8867\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27360.6016 - mae: 106.6400 - val_loss: 41525.8047 - val_mae: 110.6812\nencoding chunk 4/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 47972.8438 - mae: 119.4712 - val_loss: 38029.9570 - val_mae: 109.3613\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 40400.5625 - mae: 116.3372 - val_loss: 39592.2617 - val_mae: 110.5285\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 35723.8594 - mae: 113.0546 - val_loss: 38276.2227 - val_mae: 112.0361\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 34167.9336 - mae: 112.2732 - val_loss: 41673.2656 - val_mae: 113.6272\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 33091.3789 - mae: 111.5918 - val_loss: 41132.2930 - val_mae: 109.3832\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 30946.5547 - mae: 109.5324 - val_loss: 41201.7188 - val_mae: 115.4023\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 30027.7676 - mae: 109.5990 - val_loss: 39438.1406 - val_mae: 108.1466\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28940.2773 - mae: 107.9555 - val_loss: 40304.4570 - val_mae: 109.4971\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28479.8008 - mae: 107.8158 - val_loss: 39837.3047 - val_mae: 108.9024\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28004.1152 - mae: 108.0621 - val_loss: 42432.1641 - val_mae: 109.3874\nencoding chunk 5/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 46762.3594 - mae: 119.5305 - val_loss: 42762.2812 - val_mae: 115.4638\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 40174.0664 - mae: 116.5710 - val_loss: 42400.9375 - val_mae: 111.3235\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 36940.1523 - mae: 113.3407 - val_loss: 39905.3281 - val_mae: 110.4359\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 33204.2695 - mae: 111.5470 - val_loss: 40821.4336 - val_mae: 112.4977\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 33812.0391 - mae: 111.9874 - val_loss: 41770.9219 - val_mae: 110.7500\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 31249.1680 - mae: 109.9229 - val_loss: 40530.2969 - val_mae: 107.8499\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 30758.8730 - mae: 109.7265 - val_loss: 42868.0430 - val_mae: 113.4376\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28821.5762 - mae: 107.8893 - val_loss: 41381.1211 - val_mae: 108.4631\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28970.1348 - mae: 107.7377 - val_loss: 42371.6172 - val_mae: 113.4123\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27784.6367 - mae: 107.3566 - val_loss: 41468.4141 - val_mae: 109.9279\nencoding chunk 6/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 45434.1484 - mae: 118.1158 - val_loss: 41344.0859 - val_mae: 114.7782\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 39273.3711 - mae: 117.3896 - val_loss: 40742.1055 - val_mae: 110.3857\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 35132.1445 - mae: 113.2806 - val_loss: 40113.4453 - val_mae: 109.6404\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 33693.6406 - mae: 111.8523 - val_loss: 41784.6445 - val_mae: 110.4869\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 32430.9961 - mae: 111.0476 - val_loss: 41672.5273 - val_mae: 110.5258\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 32295.3008 - mae: 111.0897 - val_loss: 40217.6328 - val_mae: 108.7682\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 30705.0703 - mae: 109.4494 - val_loss: 41655.5859 - val_mae: 109.3861\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 29178.9336 - mae: 107.8888 - val_loss: 44465.0430 - val_mae: 113.5454\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - loss: 28056.4434 - mae: 107.2574 - val_loss: 41872.0664 - val_mae: 111.1021\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27127.0137 - mae: 107.0206 - val_loss: 43506.9805 - val_mae: 113.7219\nencoding chunk 7/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 43255.6797 - mae: 115.9520 - val_loss: 41751.7812 - val_mae: 108.7021\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 38080.3633 - mae: 114.5085 - val_loss: 43371.7383 - val_mae: 109.8622\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 34507.8750 - mae: 111.9045 - val_loss: 41101.0078 - val_mae: 108.9161\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31900.1230 - mae: 110.2087 - val_loss: 42829.6211 - val_mae: 112.0547\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30969.6016 - mae: 109.6535 - val_loss: 41543.6484 - val_mae: 109.0032\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29240.5020 - mae: 108.2735 - val_loss: 42920.0078 - val_mae: 109.0054\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28126.9883 - mae: 107.4996 - val_loss: 43096.3203 - val_mae: 109.6004\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27256.6152 - mae: 106.7744 - val_loss: 43394.3828 - val_mae: 108.6936\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26270.2773 - mae: 105.9425 - val_loss: 42208.7500 - val_mae: 107.2689\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27271.4453 - mae: 106.0090 - val_loss: 42813.6289 - val_mae: 107.6648\nencoding chunk 8/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 43540.8633 - mae: 116.0661 - val_loss: 40749.6602 - val_mae: 112.5912\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 36586.9141 - mae: 113.5779 - val_loss: 47253.6484 - val_mae: 125.0704\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 34256.4414 - mae: 112.4257 - val_loss: 43519.8047 - val_mae: 115.7059\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 32483.8340 - mae: 111.3130 - val_loss: 43002.5547 - val_mae: 110.1459\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 30465.4902 - mae: 108.5324 - val_loss: 41990.1445 - val_mae: 109.8659\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29638.9531 - mae: 108.2178 - val_loss: 42054.8984 - val_mae: 109.7171\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28260.9961 - mae: 107.8727 - val_loss: 43616.1680 - val_mae: 111.7822\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27020.5488 - mae: 106.1916 - val_loss: 45567.3359 - val_mae: 114.7359\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27129.5566 - mae: 107.0736 - val_loss: 43644.7578 - val_mae: 110.8978\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26628.6680 - mae: 106.3223 - val_loss: 43362.8125 - val_mae: 111.6753\nencoding chunk 9/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - loss: 44472.6680 - mae: 117.2185 - val_loss: 41107.1016 - val_mae: 110.3899\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 38768.6914 - mae: 115.5580 - val_loss: 39309.9531 - val_mae: 109.1430\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 36112.4570 - mae: 112.5990 - val_loss: 39563.4648 - val_mae: 107.3793\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 32763.8281 - mae: 110.2565 - val_loss: 38963.4922 - val_mae: 108.2623\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 31852.1875 - mae: 110.2704 - val_loss: 41372.5938 - val_mae: 107.5398\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29113.3203 - mae: 107.4456 - val_loss: 41527.3555 - val_mae: 112.5207\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28573.1406 - mae: 106.9645 - val_loss: 39988.4219 - val_mae: 107.5013\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28732.1895 - mae: 107.2046 - val_loss: 41209.3359 - val_mae: 110.9001\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27420.6133 - mae: 106.6531 - val_loss: 42474.1172 - val_mae: 112.1455\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27374.1211 - mae: 105.6286 - val_loss: 41380.4844 - val_mae: 111.5778\nencoding chunk 10/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - loss: 43317.6406 - mae: 116.3210 - val_loss: 45963.9727 - val_mae: 114.3497\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 39031.5430 - mae: 114.2444 - val_loss: 43790.5859 - val_mae: 110.9189\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 35785.7773 - mae: 112.2640 - val_loss: 43652.0117 - val_mae: 110.9846\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 32273.5234 - mae: 110.9955 - val_loss: 44436.3008 - val_mae: 109.8698\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 30829.3496 - mae: 109.6819 - val_loss: 44301.8086 - val_mae: 110.8563\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 30809.7676 - mae: 108.7238 - val_loss: 45701.7266 - val_mae: 108.9274\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 29498.8457 - mae: 108.4140 - val_loss: 45870.0938 - val_mae: 109.3464\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 28263.0508 - mae: 108.4679 - val_loss: 46541.9883 - val_mae: 110.2916\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27681.9766 - mae: 107.2388 - val_loss: 46156.5820 - val_mae: 110.2932\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 28052.4453 - mae: 107.1682 - val_loss: 46712.0547 - val_mae: 109.5512\nencoding chunk 1/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - loss: 39961.4961 - mae: 114.1581 - val_loss: 41022.9531 - val_mae: 109.5511\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 33904.6641 - mae: 111.2201 - val_loss: 40087.5625 - val_mae: 107.4673\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30248.4141 - mae: 108.6053 - val_loss: 40609.3047 - val_mae: 107.2216\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 29719.3008 - mae: 108.6504 - val_loss: 41248.0430 - val_mae: 109.9042\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 28442.3652 - mae: 107.5363 - val_loss: 41372.3086 - val_mae: 107.7181\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27056.7695 - mae: 105.5471 - val_loss: 39994.3594 - val_mae: 105.4767\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26377.7969 - mae: 105.2974 - val_loss: 42357.0312 - val_mae: 107.9831\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25386.6387 - mae: 104.5041 - val_loss: 40277.3164 - val_mae: 105.3570\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 25421.2715 - mae: 104.5788 - val_loss: 41701.0703 - val_mae: 105.4321\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 24787.8242 - mae: 103.2825 - val_loss: 42293.8711 - val_mae: 106.6084\nencoding chunk 2/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - loss: 42396.2773 - mae: 114.3089 - val_loss: 40458.2500 - val_mae: 109.4495\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 35089.1133 - mae: 111.3731 - val_loss: 41364.4414 - val_mae: 110.1365\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - loss: 32123.8145 - mae: 109.5025 - val_loss: 41669.4570 - val_mae: 109.3672\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 29054.6191 - mae: 107.3664 - val_loss: 41099.2070 - val_mae: 108.8787\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 29390.8887 - mae: 107.3122 - val_loss: 40382.7734 - val_mae: 106.6797\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27661.8105 - mae: 106.1806 - val_loss: 41813.1328 - val_mae: 109.0917\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26490.2871 - mae: 104.4429 - val_loss: 40447.3477 - val_mae: 106.3446\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26477.9824 - mae: 105.6479 - val_loss: 41509.3477 - val_mae: 107.6038\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 25803.4961 - mae: 104.6747 - val_loss: 41189.1055 - val_mae: 109.6504\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 25103.3906 - mae: 104.1019 - val_loss: 41530.4727 - val_mae: 107.0000\nencoding chunk 3/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - loss: 40862.6602 - mae: 114.3937 - val_loss: 38819.9492 - val_mae: 109.2828\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 34933.2422 - mae: 112.0659 - val_loss: 38627.0117 - val_mae: 108.9666\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - loss: 31047.5020 - mae: 108.9700 - val_loss: 39386.5469 - val_mae: 112.3071\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30085.0117 - mae: 107.4640 - val_loss: 38060.2031 - val_mae: 107.6900\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 28106.2656 - mae: 106.5311 - val_loss: 37704.1172 - val_mae: 106.4871\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 27251.5254 - mae: 105.6707 - val_loss: 37709.9219 - val_mae: 108.6000\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26600.6523 - mae: 104.6339 - val_loss: 40807.0391 - val_mae: 112.7934\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26649.5957 - mae: 105.6899 - val_loss: 39225.4570 - val_mae: 108.6753\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26056.9160 - mae: 104.0431 - val_loss: 38889.7344 - val_mae: 107.5531\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 25724.5684 - mae: 104.4971 - val_loss: 39703.0859 - val_mae: 107.6542\nencoding chunk 4/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - loss: 40527.4023 - mae: 113.8603 - val_loss: 39542.5508 - val_mae: 110.1082\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 34824.2344 - mae: 112.2250 - val_loss: 39933.4961 - val_mae: 108.9885\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31855.1152 - mae: 109.3736 - val_loss: 40587.9570 - val_mae: 113.5930\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30356.6992 - mae: 108.4008 - val_loss: 40259.2422 - val_mae: 107.9484\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 28917.5723 - mae: 107.1847 - val_loss: 39920.2852 - val_mae: 107.2139\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 27602.1113 - mae: 106.7169 - val_loss: 39487.3008 - val_mae: 107.2687\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26504.4590 - mae: 105.5811 - val_loss: 41481.4727 - val_mae: 111.3479\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26580.4141 - mae: 105.0161 - val_loss: 38790.9805 - val_mae: 107.1426\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 25941.5547 - mae: 104.3633 - val_loss: 40774.2266 - val_mae: 107.0777\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26080.1309 - mae: 104.3166 - val_loss: 40279.3633 - val_mae: 106.6649\nencoding chunk 5/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - loss: 42119.5938 - mae: 115.2059 - val_loss: 39363.6680 - val_mae: 107.5762\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 34078.4531 - mae: 112.0650 - val_loss: 41882.0508 - val_mae: 115.2255\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31846.4941 - mae: 109.7024 - val_loss: 38818.4492 - val_mae: 106.5883\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30244.9746 - mae: 108.8378 - val_loss: 39006.5586 - val_mae: 107.2782\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 28390.1523 - mae: 106.5624 - val_loss: 40854.9570 - val_mae: 109.6917\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27754.1777 - mae: 106.2674 - val_loss: 40055.7188 - val_mae: 106.0992\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26855.9219 - mae: 105.0251 - val_loss: 41419.6562 - val_mae: 109.6053\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26862.6211 - mae: 105.2412 - val_loss: 44405.4766 - val_mae: 113.2024\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27113.9102 - mae: 105.9970 - val_loss: 41100.0820 - val_mae: 108.0745\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26343.6035 - mae: 104.5747 - val_loss: 42123.7305 - val_mae: 106.3745\nencoding chunk 6/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - loss: 41703.4336 - mae: 115.5252 - val_loss: 41420.0625 - val_mae: 110.3605\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 34110.4922 - mae: 111.4520 - val_loss: 41386.6719 - val_mae: 109.3069\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31348.0000 - mae: 109.3517 - val_loss: 40156.4492 - val_mae: 107.1934\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30352.1660 - mae: 108.7372 - val_loss: 41640.2812 - val_mae: 108.8272\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 28439.1875 - mae: 107.4083 - val_loss: 42295.9219 - val_mae: 111.1308\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27545.2129 - mae: 106.1314 - val_loss: 42350.3594 - val_mae: 108.9222\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 27295.0254 - mae: 105.8038 - val_loss: 41665.1328 - val_mae: 108.0087\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26459.2031 - mae: 105.2056 - val_loss: 42307.6523 - val_mae: 110.0500\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25917.4473 - mae: 105.3554 - val_loss: 40943.2422 - val_mae: 107.2195\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25861.1523 - mae: 104.7699 - val_loss: 42347.4414 - val_mae: 108.1961\nencoding chunk 7/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - loss: 41148.8086 - mae: 114.1230 - val_loss: 41519.6367 - val_mae: 109.2525\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 33831.7539 - mae: 111.5341 - val_loss: 41672.3555 - val_mae: 107.0612\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31319.4727 - mae: 108.4770 - val_loss: 42242.1016 - val_mae: 106.3725\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 29120.1582 - mae: 106.9317 - val_loss: 41359.9336 - val_mae: 106.2682\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27821.0352 - mae: 106.2572 - val_loss: 40860.7383 - val_mae: 106.8230\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26771.6426 - mae: 104.9993 - val_loss: 44417.5078 - val_mae: 111.0394\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26163.9297 - mae: 104.4885 - val_loss: 43277.6172 - val_mae: 106.4234\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25391.0117 - mae: 103.9741 - val_loss: 42477.9102 - val_mae: 108.6899\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 24752.4902 - mae: 103.0674 - val_loss: 42409.9531 - val_mae: 107.0070\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 24722.0938 - mae: 102.8337 - val_loss: 42640.1406 - val_mae: 106.9582\nencoding chunk 8/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 38692.8203 - mae: 112.7089 - val_loss: 41292.0547 - val_mae: 107.5195\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 32527.8340 - mae: 110.0732 - val_loss: 41132.7148 - val_mae: 109.4252\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30205.6484 - mae: 107.6273 - val_loss: 41730.3086 - val_mae: 108.7093\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 28241.3672 - mae: 107.8017 - val_loss: 39936.9648 - val_mae: 107.7745\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27151.6816 - mae: 105.5338 - val_loss: 40787.7773 - val_mae: 108.4356\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26378.0547 - mae: 105.2191 - val_loss: 40428.4492 - val_mae: 105.5399\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26202.8516 - mae: 105.1798 - val_loss: 39755.5547 - val_mae: 109.7372\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25541.1641 - mae: 104.5312 - val_loss: 42165.7891 - val_mae: 108.5100\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 24823.2812 - mae: 103.4197 - val_loss: 40930.8828 - val_mae: 106.1017\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 24759.0078 - mae: 102.7810 - val_loss: 39895.6562 - val_mae: 106.3591\nencoding chunk 9/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - loss: 41079.8711 - mae: 114.9630 - val_loss: 38827.7344 - val_mae: 108.3713\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 34434.7461 - mae: 110.1782 - val_loss: 38177.7422 - val_mae: 106.3532\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30826.5859 - mae: 108.1432 - val_loss: 38734.6523 - val_mae: 105.1527\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 28755.1758 - mae: 106.9213 - val_loss: 38524.9297 - val_mae: 106.1983\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27712.4004 - mae: 106.1232 - val_loss: 38267.6562 - val_mae: 105.6168\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25919.0078 - mae: 104.6426 - val_loss: 38910.2695 - val_mae: 105.2868\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25318.4902 - mae: 103.7224 - val_loss: 38827.9219 - val_mae: 105.2299\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25606.0938 - mae: 104.0066 - val_loss: 39221.8359 - val_mae: 105.8510\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25438.1211 - mae: 102.7832 - val_loss: 39068.5273 - val_mae: 105.2934\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25055.7090 - mae: 103.7079 - val_loss: 41635.8750 - val_mae: 108.9863\nencoding chunk 10/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - loss: 40111.5625 - mae: 114.2486 - val_loss: 43834.5430 - val_mae: 108.8499\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 32506.9746 - mae: 109.4421 - val_loss: 44044.5859 - val_mae: 113.6908\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30609.7480 - mae: 108.6365 - val_loss: 45260.4023 - val_mae: 110.3211\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 28241.6172 - mae: 106.4491 - val_loss: 44550.5117 - val_mae: 108.4892\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 27684.0684 - mae: 106.2436 - val_loss: 46230.6211 - val_mae: 114.3679\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26912.8496 - mae: 105.8482 - val_loss: 44826.0859 - val_mae: 109.4790\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25523.9551 - mae: 105.0047 - val_loss: 45556.2969 - val_mae: 108.4728\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25627.8789 - mae: 104.2725 - val_loss: 44876.6836 - val_mae: 107.0690\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25501.6992 - mae: 104.3618 - val_loss: 45558.6133 - val_mae: 109.8561\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26560.4531 - mae: 104.7480 - val_loss: 45632.6641 - val_mae: 109.7110\nencoding chunk 1/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - loss: 35526.6484 - mae: 110.3926 - val_loss: 39296.5703 - val_mae: 106.2580\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30847.3066 - mae: 108.1110 - val_loss: 38650.3008 - val_mae: 104.9220\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 28275.2695 - mae: 106.1997 - val_loss: 39405.0898 - val_mae: 109.0595\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 27127.4922 - mae: 104.7209 - val_loss: 40073.4258 - val_mae: 106.4043\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25619.7695 - mae: 103.7532 - val_loss: 40469.0703 - val_mae: 105.4903\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25675.4590 - mae: 103.5101 - val_loss: 38876.8750 - val_mae: 104.3861\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 26103.0801 - mae: 103.5678 - val_loss: 39212.0195 - val_mae: 104.7853\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 24116.7461 - mae: 101.9921 - val_loss: 42262.2500 - val_mae: 112.8790\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 23854.8203 - mae: 101.5323 - val_loss: 39981.9180 - val_mae: 104.0729\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 23696.7656 - mae: 101.8686 - val_loss: 41063.7188 - val_mae: 109.9800\nencoding chunk 2/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - loss: 35455.1250 - mae: 110.2225 - val_loss: 40155.3828 - val_mae: 106.4552\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31024.8945 - mae: 108.5654 - val_loss: 47616.7188 - val_mae: 128.8672\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - loss: 29100.9121 - mae: 107.2657 - val_loss: 40209.0469 - val_mae: 107.6209\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 27129.4863 - mae: 105.2878 - val_loss: 43178.4688 - val_mae: 117.1736\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26671.3125 - mae: 105.1090 - val_loss: 39313.1484 - val_mae: 104.3157\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26146.1504 - mae: 104.5043 - val_loss: 41548.1523 - val_mae: 110.0818\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 25792.0156 - mae: 103.9149 - val_loss: 41331.8789 - val_mae: 107.8955\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 23992.3750 - mae: 102.1765 - val_loss: 41052.3906 - val_mae: 105.7384\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 24140.2988 - mae: 102.3832 - val_loss: 39914.2461 - val_mae: 103.6681\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 24080.2227 - mae: 101.8560 - val_loss: 44522.5781 - val_mae: 115.0488\nencoding chunk 3/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - loss: 36218.5859 - mae: 111.1012 - val_loss: 40329.7656 - val_mae: 112.8074\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 32419.5977 - mae: 109.1836 - val_loss: 38626.7344 - val_mae: 106.5494\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 29563.8223 - mae: 106.9975 - val_loss: 38533.6641 - val_mae: 105.8766\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 27966.3203 - mae: 106.3410 - val_loss: 38925.9922 - val_mae: 105.6201\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26717.1699 - mae: 104.6955 - val_loss: 39909.0000 - val_mae: 107.5630\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26090.4824 - mae: 103.9358 - val_loss: 39672.8203 - val_mae: 105.8422\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25366.5508 - mae: 103.4218 - val_loss: 39509.3867 - val_mae: 106.9084\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 25305.9863 - mae: 103.2854 - val_loss: 40321.6211 - val_mae: 106.5225\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 24389.4883 - mae: 102.4621 - val_loss: 41788.9219 - val_mae: 109.6356\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 24861.7441 - mae: 102.5891 - val_loss: 40132.5547 - val_mae: 110.0189\nencoding chunk 4/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - loss: 38472.2227 - mae: 112.7013 - val_loss: 38250.3594 - val_mae: 107.5251\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31911.3652 - mae: 109.0505 - val_loss: 37891.0469 - val_mae: 106.7478\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30788.7441 - mae: 107.1274 - val_loss: 38352.9844 - val_mae: 106.5620\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 28119.1035 - mae: 105.9486 - val_loss: 37456.0703 - val_mae: 104.6945\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 27428.3105 - mae: 105.7454 - val_loss: 38648.1523 - val_mae: 108.8623\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26389.0801 - mae: 104.4451 - val_loss: 37819.3789 - val_mae: 107.2888\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26631.2500 - mae: 104.0287 - val_loss: 36984.0586 - val_mae: 104.0304\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 25705.4238 - mae: 103.3277 - val_loss: 40379.5000 - val_mae: 112.4158\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 25018.1113 - mae: 102.8703 - val_loss: 38148.3008 - val_mae: 105.1321\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 24328.5586 - mae: 102.3349 - val_loss: 37281.5352 - val_mae: 105.6199\nencoding chunk 5/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - loss: 37325.9336 - mae: 111.9678 - val_loss: 41773.5352 - val_mae: 112.6278\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31859.1113 - mae: 108.9513 - val_loss: 39124.5430 - val_mae: 107.9188\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 31104.8086 - mae: 108.1728 - val_loss: 39709.0820 - val_mae: 106.3895\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 28424.6973 - mae: 106.4479 - val_loss: 38744.4648 - val_mae: 104.9231\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26758.6504 - mae: 104.7553 - val_loss: 40769.9922 - val_mae: 106.9432\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26442.0195 - mae: 104.0435 - val_loss: 39509.1719 - val_mae: 105.8968\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25507.9844 - mae: 103.4035 - val_loss: 40894.3086 - val_mae: 107.6389\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25273.9355 - mae: 103.1836 - val_loss: 39174.9414 - val_mae: 104.9665\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 24259.9219 - mae: 102.7583 - val_loss: 39982.0781 - val_mae: 105.3374\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 24551.6113 - mae: 102.8373 - val_loss: 40968.5938 - val_mae: 104.4796\nencoding chunk 6/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - loss: 36940.3906 - mae: 111.7200 - val_loss: 41244.4648 - val_mae: 108.3423\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30813.3184 - mae: 108.3346 - val_loss: 40912.9922 - val_mae: 106.7968\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 28643.4121 - mae: 106.7955 - val_loss: 40876.0000 - val_mae: 108.5488\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 27614.7324 - mae: 106.0341 - val_loss: 42552.3086 - val_mae: 107.7228\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26323.7598 - mae: 105.3285 - val_loss: 42456.4180 - val_mae: 107.4407\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25411.5605 - mae: 103.9893 - val_loss: 41725.9648 - val_mae: 106.3373\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25443.0449 - mae: 103.7782 - val_loss: 42574.6289 - val_mae: 106.5698\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 24996.3125 - mae: 103.9375 - val_loss: 42567.5938 - val_mae: 107.4803\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 24901.2168 - mae: 102.9471 - val_loss: 41647.8867 - val_mae: 106.2332\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 24638.8848 - mae: 102.8530 - val_loss: 42150.5547 - val_mae: 107.0564\nencoding chunk 7/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - loss: 36961.9648 - mae: 111.3862 - val_loss: 43922.3750 - val_mae: 117.5948\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30871.6230 - mae: 108.0462 - val_loss: 42959.4062 - val_mae: 107.1096\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 28055.3496 - mae: 106.0767 - val_loss: 40817.3789 - val_mae: 104.2053\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 27143.4336 - mae: 105.2823 - val_loss: 42745.8516 - val_mae: 105.6568\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26986.2734 - mae: 104.3573 - val_loss: 40667.4844 - val_mae: 105.6724\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 26619.0273 - mae: 104.5558 - val_loss: 41639.4414 - val_mae: 104.8814\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 25059.2480 - mae: 102.6811 - val_loss: 41840.7305 - val_mae: 104.2851\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25321.7520 - mae: 103.5662 - val_loss: 40540.0078 - val_mae: 104.4396\nEpoch 9/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 24433.6016 - mae: 102.2380 - val_loss: 40933.4531 - val_mae: 104.2227\nEpoch 10/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 24265.5039 - mae: 102.1912 - val_loss: 40872.6602 - val_mae: 103.9472\nencoding chunk 8/10..  (shape = 198026)\nencoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - loss: 35552.8320 - mae: 110.5624 - val_loss: 42707.8477 - val_mae: 117.1972\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30240.7227 - mae: 108.4351 - val_loss: 39492.5781 - val_mae: 105.1119\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 27650.7676 - mae: 106.1132 - val_loss: 41685.8008 - val_mae: 110.5200\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26959.5039 - mae: 105.3224 - val_loss: 39695.6289 - val_mae: 104.9003\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25983.8750 - mae: 104.1744 - val_loss: 40679.0000 - val_mae: 106.0334\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25522.4316 - mae: 103.9417 - val_loss: 41875.6484 - val_mae: 106.1167\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 24445.5020 - mae: 103.0942 - val_loss: 42542.3398 - val_mae: 107.2405\nEpoch 8/10\n\u001b[1m 45/155\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - loss: 24072.3633 - mae: 101.8367encoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - loss: 37253.0625 - mae: 111.6275 - val_loss: 40270.8867 - val_mae: 109.0483\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30507.4570 - mae: 107.2192 - val_loss: 39249.6445 - val_mae: 110.3178\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 27645.2441 - mae: 106.0212 - val_loss: 38510.3203 - val_mae: 105.7113\nEpoch 4/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 26800.3887 - mae: 104.8580 - val_loss: 39066.0742 - val_mae: 103.8326\nEpoch 5/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25822.1816 - mae: 103.7584 - val_loss: 39034.1133 - val_mae: 104.0458\nEpoch 6/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - loss: 25466.3867 - mae: 103.0109 - val_loss: 38641.0469 - val_mae: 103.6625\nEpoch 7/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 24746.9590 - mae: 102.5083 - val_loss: 40655.5352 - val_mae: 109.5819\nEpoch 8/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - loss: 24531.6738 - mae: 102.2267 - val_loss: 38757.2344 - val_mae: 103.2510\nEpoch 9/10\n\u001b[1m 19/155\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 61ms/step - loss: 22704.9414 - mae: 100.6665encoded!\ntraining model...\nEpoch 1/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - loss: 36389.3359 - mae: 110.5382 - val_loss: 44155.2031 - val_mae: 108.7607\nEpoch 2/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 30970.6602 - mae: 108.2207 - val_loss: 43176.2734 - val_mae: 107.5843\nEpoch 3/10\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - loss: 28584.3086 - mae: 106.1117 - val_loss: 43613.2656 - val_mae: 105.8794\nEpoch 4/10\n\u001b[1m 88/155\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 26163.7480 - mae: 104.8840","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# model1.save(\"model_8814_42.h5\")  #model that was tranined only on chess1","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-01T14:46:00.798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" model2.save(\"model_8814_42_train21.h5\") ## saves the model trained on chess1 and chess2","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-02-01T13:28:31.521717Z","shell.execute_reply.started":"2025-02-01T13:28:31.206443Z","shell.execute_reply":"2025-02-01T13:28:31.521029Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# testing the model","metadata":{}},{"cell_type":"code","source":"total_mse = 0\ntotal_mae = 0\n\ntest_data = pd.read_csv(\"/kaggle/input/training-data/Chess_3.csv\")\ntest_data = processData(test_data)\n\ndata_splits = np.array_split(test_data, 20)\n\nmodel_loaded = tensorflow.keras.models.load_model(\"/kaggle/input/8814-on-chess2-and-chess1/keras/default/1/model_8814_42_train21.h5\", custom_objects={'mse': tensorflow.keras.metrics.MeanSquaredError()})\nprint(\"model loaded!\")\n\nfor index in range(len(data_splits)):\n    test = data_splits[index]\n    y_test = test['Evaluation'].values\n    encoded_fens_train = test['FEN'].apply(encoding8814)\n    x_test= np.stack(encoded_fens_train.to_list())\n    print(f\"loaded! size =({test.shape[0]})\")\n    x_additional = np.stack(test['FEN'].apply(encode_additional_features).to_list())\n    loss, mae = model_loaded.evaluate([x_test, x_additional], y_test, verbose = 1)\n    total_mse += loss*test.shape[0]\n    total_mae += mae*test.shape[0]\n    print(loss, mae)\n\ntotal_mse /= test_data.shape[0]\ntotal_mae /= test_data.shape[0]\nprint(f\"mse = {total_mse}, mae = {total_mae}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T13:38:58.534432Z","iopub.execute_input":"2025-02-01T13:38:58.534740Z","iopub.status.idle":"2025-02-01T13:57:24.431446Z","shell.execute_reply.started":"2025-02-01T13:38:58.534713Z","shell.execute_reply":"2025-02-01T13:57:24.430599Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"},{"name":"stdout","text":"model loaded!\nloaded! size =(99030)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 36211.0703 - mean_absolute_error: 109.6359\n38962.01171875 110.27685546875\nloaded! size =(99030)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 41195.8711 - mean_absolute_error: 111.4067\n41393.57421875 111.10804748535156\nloaded! size =(99030)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 41595.8242 - mean_absolute_error: 112.5191\n41690.390625 112.08338928222656\nloaded! size =(99030)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 41846.8594 - mean_absolute_error: 111.9308\n41874.9765625 111.79689025878906\nloaded! size =(99030)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 42171.1211 - mean_absolute_error: 112.2683\n42198.07421875 111.6853256225586\nloaded! size =(99030)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 42473.1797 - mean_absolute_error: 113.1800\n42657.421875 112.67457580566406\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 42782.1797 - mean_absolute_error: 111.3339\n42777.8125 111.626220703125\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 42814.5625 - mean_absolute_error: 111.5151\n42845.69140625 111.88607025146484\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43007.5000 - mean_absolute_error: 111.9460\n42999.4296875 111.59207153320312\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43007.2305 - mean_absolute_error: 111.8381\n43002.578125 111.39514923095703\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43006.5586 - mean_absolute_error: 113.4905\n43042.4765625 112.57644653320312\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43014.5469 - mean_absolute_error: 110.6148\n42995.62890625 111.49127197265625\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43073.2188 - mean_absolute_error: 112.0647\n43083.8515625 112.2032241821289\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43092.4141 - mean_absolute_error: 111.0873\n43106.9765625 111.6124267578125\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43181.7305 - mean_absolute_error: 112.2035\n43206.3125 112.3536605834961\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43357.1836 - mean_absolute_error: 111.9021\n43357.8671875 111.6067886352539\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43324.5742 - mean_absolute_error: 110.9633\n43314.0 111.32655334472656\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43335.3789 - mean_absolute_error: 112.2777\n43388.06640625 112.84233093261719\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43477.9453 - mean_absolute_error: 112.8664\n43504.6328125 112.97386932373047\nloaded! size =(99029)\n\u001b[1m3095/3095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 43526.6367 - mean_absolute_error: 111.7671\n43501.75390625 111.38973236083984\nmse = 42645.1727851118, mae = 111.82504434433982\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# total_mse = 0\n# total_mae = 0\n\n# test_data = pd.read_csv(\"/kaggle/input/training-data/Chess_3.csv\")\n# test_data = processData(test_data)\n\n# data_splits = np.array_split(test_data, 20)\n\n# model_loaded = tensorflow.keras.models.load_model(\"/kaggle/input/loadmodel/keras/default/1/model_8814_42.h5\", custom_objects={'mse': tensorflow.keras.metrics.MeanSquaredError()})\n# print(\"model loaded!\")\n\n# for index in range(len(data_splits)):\n#     test = data_splits[index]\n#     y_test = test['Evaluation'].values\n#     encoded_fens_train = test['FEN'].apply(encoding8814)\n#     x_test= np.stack(encoded_fens_train.to_list())\n#     print(f\"loaded! size =({test.shape[0]})\")\n#     x_additional = np.stack(test['FEN'].apply(encode_additional_features).to_list())\n#     loss, mae = model_loaded.evaluate([x_test, x_additional], y_test, verbose = 1)\n#     total_mse += loss*test.shape[0]\n#     total_mae += mae*test.shape[0]\n#     print(loss, mae)\n\n# total_mse /= test_data.shape[0]\n# total_mae /= test_data.shape[0]\n# print(f\"mse = {total_mse}, mae = {total_mae}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T13:35:15.239399Z","iopub.status.idle":"2025-02-01T13:35:15.239636Z","shell.execute_reply":"2025-02-01T13:35:15.239541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Making Submission!","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow.keras.models\n\n\ndef processTest(test_data):\n    test_data['FEN'] = test_data['FEN'].apply(convert_fen)\n    return test_data\n\n\ndef makeSubmission(test_data, split_size):\n    \n    data_splits = np.array_split(test_data, split_size)\n\n    predictions = []\n\n    for index, test_chunk in enumerate(data_splits):\n        \n        print(f\"loading chunk {index + 1}/{split_size} (size={test_chunk.shape[0]})\")\n\n        test_chunk = processTest(test_chunk)\n        \n        encoded_fens_test = test_chunk['FEN'].apply(encoding8814)\n        x_test = np.stack(encoded_fens_test.to_list())\n\n        x_additional = np.stack(test_chunk['FEN'].apply(encode_additional_features).to_list())\n    \n        chunk_predictions = model_loaded.predict([x_test, x_additional], verbose=1)\n\n        print(\"predicting...\")\n        predictions.extend(chunk_predictions.flatten())\n        \n    return predictions\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T14:02:07.038506Z","iopub.execute_input":"2025-02-01T14:02:07.038813Z","iopub.status.idle":"2025-02-01T14:02:07.050963Z","shell.execute_reply.started":"2025-02-01T14:02:07.038790Z","shell.execute_reply":"2025-02-01T14:02:07.050304Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"model_loaded = tensorflow.keras.models.load_model(\"/kaggle/input/8814-on-chess2-and-chess1/keras/default/1/model_8814_42_train21.h5\", custom_objects={'mse': tensorflow.keras.metrics.MeanSquaredError()})\nprint(\"model loaded!\")\n\ntest_data = pd.read_csv(\"/kaggle/input/test-data-set/test_track_8.csv\")\ntest_data.rename(columns={'ID': 'FEN'}, inplace=True)\n\npredictions = makeSubmission(test_data, 20)\n\nsumbission_df = pd.DataFrame({\"FEN\": test_data[\"FEN\"], \"Evaluation\": predictions})\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T14:02:35.136444Z","iopub.execute_input":"2025-02-01T14:02:35.136724Z","iopub.status.idle":"2025-02-01T14:20:32.345532Z","shell.execute_reply.started":"2025-02-01T14:02:35.136700Z","shell.execute_reply":"2025-02-01T14:20:32.344764Z"}},"outputs":[{"name":"stdout","text":"model loaded!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"},{"name":"stdout","text":"loading chunk 1/20 (size=98970)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step\npredicting...\nloading chunk 2/20 (size=98970)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 3/20 (size=98970)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 4/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step\npredicting...\nloading chunk 5/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 6/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 7/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 8/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 9/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step\npredicting...\nloading chunk 10/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 11/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 12/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 13/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 14/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 15/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 16/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 17/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 18/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 19/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\nloading chunk 20/20 (size=98969)\n\u001b[1m3093/3093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\npredicting...\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print(sumbission_df.head(10))\n\nsumbission_df[\"Evaluation\"] = sumbission_df.apply(flip_eval, axis=1)\n\nsumbission_df.rename(columns={'FEN': 'ID'}, inplace=True)\n\nprint(\"\\n\\n\")\n\nprint(sumbission_df.head(10))\n\nsumbission_df.to_csv(\"submission3.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T14:21:39.115221Z","iopub.execute_input":"2025-02-01T14:21:39.115500Z","iopub.status.idle":"2025-02-01T14:21:59.114394Z","shell.execute_reply.started":"2025-02-01T14:21:39.115479Z","shell.execute_reply":"2025-02-01T14:21:59.113703Z"}},"outputs":[{"name":"stdout","text":"                                                 FEN  Evaluation\n0  r3k2r/pp1qn1pp/2nbpp2/2pp4/5N2/4P3/PPPPQPPP/RN...  -65.945526\n1            1Q6/2R5/4pk2/6q1/P7/1K6/8/7q b - - 0 42  489.331116\n2  r4rk1/pp1b1qpp/3Pp2n/2B1p3/8/P5NP/1P2QPP1/2R1K... -421.389404\n3                8/8/4k3/pKp2r2/2P5/8/8/8 b - - 3 53  543.987671\n4  q3kbnr/1b1p1ppp/4p3/1p2n3/2p1P3/2PP4/1P2QPPP/1...   16.349640\n5  rnbqk2r/pppp1ppp/5n2/4p3/2B1P3/2N2P2/PPPP2PP/R...  -26.700813\n6  1r4k1/4q1p1/3p1p1p/prpPpP1Q/8/P3P2P/1P1R1RP1/6...   52.096733\n7  7r/p1k4P/1p6/2p1b1P1/P1P5/4PB2/5PK1/7R w - - 1 41  550.215576\n8       8/1p1N4/1p4p1/1P1P3k/1r6/p3K3/8/8 w - - 2 49 -500.272095\n9  r4br1/ppk2p2/2p2p1p/6p1/4N3/P4P1P/1PP2P2/3R1RK...  175.039429\n\n\n\n                                                  ID  Evaluation\n0  r3k2r/pp1qn1pp/2nbpp2/2pp4/5N2/4P3/PPPPQPPP/RN...  -65.945526\n1            1Q6/2R5/4pk2/6q1/P7/1K6/8/7q b - - 0 42 -489.331116\n2  r4rk1/pp1b1qpp/3Pp2n/2B1p3/8/P5NP/1P2QPP1/2R1K...  421.389404\n3                8/8/4k3/pKp2r2/2P5/8/8/8 b - - 3 53 -543.987671\n4  q3kbnr/1b1p1ppp/4p3/1p2n3/2p1P3/2PP4/1P2QPPP/1...   16.349640\n5  rnbqk2r/pppp1ppp/5n2/4p3/2B1P3/2N2P2/PPPP2PP/R...   26.700813\n6  1r4k1/4q1p1/3p1p1p/prpPpP1Q/8/P3P2P/1P1R1RP1/6...   52.096733\n7  7r/p1k4P/1p6/2p1b1P1/P1P5/4PB2/5PK1/7R w - - 1 41  550.215576\n8       8/1p1N4/1p4p1/1P1P3k/1r6/p3K3/8/8 w - - 2 49 -500.272095\n9  r4br1/ppk2p2/2p2p1p/6p1/4N3/P4P1P/1PP2P2/3R1RK... -175.039429\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}